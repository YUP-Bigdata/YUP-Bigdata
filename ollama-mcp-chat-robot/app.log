2025-06-13 08:37:36,972 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 08:37:36,992 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 08:37:42,226 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-13 08:40:04,798 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 08:40:04,818 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 08:40:10,543 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-13 08:49:25,275 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 08:49:25,294 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 08:49:31,988 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-13 08:49:34,163 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-13 08:49:34,164 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7b8f52965400>
2025-06-13 08:49:34,164 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-13 08:49:34,164 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-13 08:49:34,164 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-13 08:49:34,164 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-13 08:49:34,164 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-13 08:54:19,739 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Fri, 13 Jun 2025 08:54:19 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-13 08:54:19,740 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-13 08:54:19,740 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-13 08:54:19,740 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-13 08:54:19,740 - httpcore.http11 - DEBUG - response_closed.started
2025-06-13 08:54:19,741 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-13 08:54:19,792 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.048s]
2025-06-13 08:57:47,119 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 08:57:47,139 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 08:57:52,286 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-13 08:57:53,172 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-13 08:57:53,172 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e086b19ddf0>
2025-06-13 08:57:53,173 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-13 08:57:53,173 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-13 08:57:53,173 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-13 08:57:53,173 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-13 08:57:53,173 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-13 09:03:24,927 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Fri, 13 Jun 2025 09:03:24 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-13 09:03:24,928 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-13 09:03:24,928 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-13 09:03:24,929 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-13 09:03:24,929 - httpcore.http11 - DEBUG - response_closed.started
2025-06-13 09:03:24,929 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-13 09:03:25,000 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.068s]
2025-06-13 09:06:30,936 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 09:06:30,938 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 09:06:38,571 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-13 09:06:46,154 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-13 09:06:46,155 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7607d7f9e5a0>
2025-06-13 09:06:46,155 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-13 09:06:46,155 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-13 09:06:46,155 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-13 09:06:46,155 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-13 09:06:46,155 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-13 09:09:44,631 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Fri, 13 Jun 2025 09:09:44 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-13 09:09:44,632 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-13 09:09:44,632 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-13 09:09:44,632 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-13 09:09:44,632 - httpcore.http11 - DEBUG - response_closed.started
2025-06-13 09:09:44,632 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-13 09:09:44,639 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 282, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-13 09:14:59,276 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 09:14:59,278 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 09:15:04,428 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-13 09:15:37,510 - api.api_server - ERROR - 오류 발생: OllamaAgentManager.chat() got an unexpected keyword argument 'messages'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 282, in chat_endpoint
    result = await agent_manager.chat(
                   ^^^^^^^^^^^^^^^^^^^
TypeError: OllamaAgentManager.chat() got an unexpected keyword argument 'messages'
2025-06-13 09:15:46,967 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 09:15:46,969 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-13 09:15:54,553 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-13 09:15:56,398 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-13 09:15:56,399 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x702369f9e510>
2025-06-13 09:15:56,399 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-13 09:15:56,399 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-13 09:15:56,399 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-13 09:15:56,399 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-13 09:15:56,399 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-13 09:18:59,809 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Fri, 13 Jun 2025 09:18:59 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-13 09:18:59,809 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-13 09:18:59,809 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-13 09:18:59,809 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-13 09:18:59,809 - httpcore.http11 - DEBUG - response_closed.started
2025-06-13 09:18:59,810 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-13 09:18:59,816 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 282, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 00:11:04,208 - httpcore.connection - DEBUG - close.started
2025-06-16 00:11:04,209 - httpcore.connection - DEBUG - close.complete
2025-06-16 00:11:04,209 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 00:11:04,210 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x70236a92e5d0>
2025-06-16 00:11:04,211 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 00:11:04,211 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 00:11:04,211 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 00:11:04,212 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 00:11:04,212 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 00:16:54,342 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 00:16:54 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 00:16:54,342 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 00:16:54,343 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 00:16:54,343 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 00:16:54,343 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 00:16:54,343 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 00:16:54,423 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.078s]
2025-06-16 00:16:54,477 - api.api_server - ERROR - 오류 발생: unhandled errors in a TaskGroup (1 sub-exception)
  + Exception Group Traceback (most recent call last):
  |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 112, in shutdown_event
  |     await mcp_client.__aexit__(None, None, None)
  |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py", line 370, in __aexit__
  |     await self.exit_stack.aclose()
  |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/contextlib.py", line 696, in aclose
  |     await self.__aexit__(None, None, None)
  |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/contextlib.py", line 754, in __aexit__
  |     raise exc_details[1]
  |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/contextlib.py", line 737, in __aexit__
  |     cb_suppress = await cb(*exc_details)
  |                   ^^^^^^^^^^^^^^^^^^^^^^
  |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/contextlib.py", line 231, in __aexit__
  |     await self.gen.athrow(value)
  |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/mcp/client/stdio/__init__.py", line 167, in stdio_client
  |     anyio.create_task_group() as tg,
  |     ^^^^^^^^^^^^^^^^^^^^^^^^^
  |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py", line 772, in __aexit__
  |     raise BaseExceptionGroup(
  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Exception Group Traceback (most recent call last):
    |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/mcp/client/stdio/__init__.py", line 173, in stdio_client
    |     yield read_stream, write_stream
    |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/contextlib.py", line 737, in __aexit__
    |     cb_suppress = await cb(*exc_details)
    |                   ^^^^^^^^^^^^^^^^^^^^^^
    |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/mcp/shared/session.py", line 210, in __aexit__
    |     return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py", line 772, in __aexit__
    |     raise BaseExceptionGroup(
    | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
    +-+---------------- 1 ----------------
      | Exception Group Traceback (most recent call last):
      |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/contextlib.py", line 737, in __aexit__
      |     cb_suppress = await cb(*exc_details)
      |                   ^^^^^^^^^^^^^^^^^^^^^^
      |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/contextlib.py", line 217, in __aexit__
      |     await anext(self.gen)
      |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/mcp/client/stdio/__init__.py", line 167, in stdio_client
      |     anyio.create_task_group() as tg,
      |     ^^^^^^^^^^^^^^^^^^^^^^^^^
      |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py", line 772, in __aexit__
      |     raise BaseExceptionGroup(
      | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
      +-+---------------- 1 ----------------
        | Traceback (most recent call last):
        |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/mcp/client/stdio/__init__.py", line 179, in stdio_client
        |     process.terminate()
        |   File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py", line 1085, in terminate
        |     self._process.terminate()
        |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/asyncio/subprocess.py", line 143, in terminate
        |     self._transport.terminate()
        |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/asyncio/base_subprocess.py", line 149, in terminate
        |     self._check_proc()
        |   File "/home/ljy/anaconda3/envs/mcp/lib/python3.12/asyncio/base_subprocess.py", line 142, in _check_proc
        |     raise ProcessLookupError()
        | ProcessLookupError
        +------------------------------------
2025-06-16 00:16:55,058 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:16:55,060 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:17:00,928 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:17:00,938 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 00:17:00,939 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 00:17:00,939 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e0f406135c0>
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 00:17:00,940 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e0f3f1d14f0>
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 00:17:00,940 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 00:17:33,005 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:17:33,661 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:17:33,688 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:17:39,452 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:17:40,433 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 00:17:40,434 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a1047571520>
2025-06-16 00:17:40,434 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 00:17:40,434 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 00:17:40,434 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 00:17:40,434 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 00:17:40,434 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 00:23:19,286 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 00:23:19 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 00:23:19,286 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 00:23:19,286 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 00:23:19,286 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 00:23:19,286 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 00:23:19,287 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 00:23:19,293 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 282, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 00:24:50,596 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:24:50,615 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:24:55,626 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:25:19,884 - api.api_server - ERROR - 오류 발생: OllamaAgentManager.chat() got an unexpected keyword argument 'messages'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 286, in chat_endpoint
    result = await agent_manager.chat(
                   ^^^^^^^^^^^^^^^^^^^
TypeError: OllamaAgentManager.chat() got an unexpected keyword argument 'messages'
2025-06-16 00:26:34,002 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:26:34,021 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:26:38,998 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:26:40,040 - api.api_server - ERROR - 오류 발생: OllamaAgentManager.chat() got an unexpected keyword argument 'system_prompt'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 288, in chat_endpoint
    result = await agent_manager.chat(
                   ^^^^^^^^^^^^^^^^^^^
TypeError: OllamaAgentManager.chat() got an unexpected keyword argument 'system_prompt'
2025-06-16 00:26:50,409 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:26:50,430 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:26:55,966 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:26:55,973 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 00:26:55,973 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7d551819d6a0>
2025-06-16 00:26:55,974 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 00:26:55,974 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 00:26:55,974 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 00:26:55,974 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 00:26:55,974 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 00:29:53,668 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 00:29:53 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 00:29:53,669 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 00:29:53,669 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 00:29:53,669 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 00:29:53,669 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 00:29:53,669 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 00:29:53,673 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 282, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 00:30:35,882 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:30:35,900 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:30:43,442 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:30:44,447 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 00:30:44,448 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x791ad0fae540>
2025-06-16 00:30:44,448 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 00:30:44,449 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 00:30:44,449 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 00:30:44,449 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 00:30:44,449 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 00:36:14,707 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 00:36:14 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 00:36:14,708 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 00:36:14,708 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 00:36:14,709 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 00:36:14,709 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 00:36:14,709 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 00:36:15,670 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.959s]
2025-06-16 00:36:17,252 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:36:17,271 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:36:22,267 - api.api_server - ERROR - 오류 발생: type object 'OllamaAgentManager' has no attribute 'QWEN3'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 82, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 52, in createChatModel
    model=OllamaAgentManager.QWEN3,
          ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: type object 'OllamaAgentManager' has no attribute 'QWEN3'
2025-06-16 00:38:10,745 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:38:10,763 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:38:16,434 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:38:31,974 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:38:31,993 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:38:36,898 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:40:47,969 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:40:47,987 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:40:53,058 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:42:40,147 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:42:40,165 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:42:50,523 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:44:45,897 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:44:45,918 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:44:50,719 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 00:44:50,727 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 00:44:50,727 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x70815813f6e0>
2025-06-16 00:44:50,728 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 00:44:50,728 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 00:44:50,728 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 00:44:50,728 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 00:44:50,728 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 00:53:05,251 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 00:53:05 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 00:53:05,251 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 00:53:05,251 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 00:53:05,252 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 00:53:05,252 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 00:53:05,252 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 00:53:05,304 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.050s]
2025-06-16 00:53:06,355 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:53:06,358 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 00:53:11,923 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 01:01:47,476 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:01:47,478 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:01:52,586 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 01:01:54,108 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 01:01:54,108 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e7cb9ba23f0>
2025-06-16 01:01:54,109 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 01:01:54,109 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 01:01:54,109 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 01:01:54,109 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 01:01:54,109 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 01:07:11,239 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 01:07:11 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 01:07:11,239 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 01:07:11,239 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 01:07:11,240 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 01:07:11,240 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 01:07:11,240 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 01:07:11,594 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.351s]
2025-06-16 01:07:45,301 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:07:45,303 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:07:50,458 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 01:07:54,085 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 01:07:54,085 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7d0d2c99a5d0>
2025-06-16 01:07:54,085 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 01:07:54,086 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 01:07:54,086 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 01:07:54,086 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 01:07:54,086 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 01:13:44,001 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 01:13:44 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 01:13:44,001 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 01:13:44,001 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 01:13:44,002 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 01:13:44,002 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 01:13:44,002 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 01:13:44,068 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.064s]
2025-06-16 01:17:08,044 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:17:08,046 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:17:13,104 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 01:17:16,299 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 01:17:16,300 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7b67ce59a6f0>
2025-06-16 01:17:16,300 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 01:17:16,300 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 01:17:16,300 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 01:17:16,300 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 01:17:16,300 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 01:21:59,673 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 01:21:59 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 01:21:59,674 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 01:21:59,674 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 01:21:59,674 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 01:21:59,674 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 01:21:59,674 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 01:21:59,966 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.289s]
2025-06-16 01:22:01,327 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:22:01,330 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:22:06,424 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 01:22:06,433 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 01:22:06,434 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x774aaa56da00>
2025-06-16 01:22:06,434 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 01:22:06,434 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 01:22:06,434 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 01:22:06,434 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 01:22:06,434 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 01:26:33,267 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 01:26:33 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 01:26:33,267 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 01:26:33,268 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 01:26:33,268 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 01:26:33,268 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 01:26:33,269 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 01:26:33,566 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.295s]
2025-06-16 01:36:00,538 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:36:00,541 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:36:05,639 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 01:36:06,478 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 01:36:06,479 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7395647923c0>
2025-06-16 01:36:06,479 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 01:36:06,479 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 01:36:06,479 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 01:36:06,479 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 01:36:06,479 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 01:41:17,671 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 01:41:17 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 01:41:17,672 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 01:41:17,672 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 01:41:17,673 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 01:41:17,673 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 01:41:17,673 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 01:41:17,729 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.054s]
2025-06-16 01:46:22,309 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:46:22,311 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:46:28,366 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 01:46:47,726 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 01:46:47,727 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f4f63ba1dc0>
2025-06-16 01:46:47,727 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 01:46:47,728 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 01:46:47,728 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 01:46:47,728 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 01:46:47,728 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 01:49:51,965 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 01:49:51 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 01:49:51,966 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 01:49:51,966 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 01:49:51,966 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 01:49:51,966 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 01:49:51,966 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 01:49:51,973 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 258, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 01:50:47,951 - httpcore.connection - DEBUG - close.started
2025-06-16 01:50:47,951 - httpcore.connection - DEBUG - close.complete
2025-06-16 01:50:47,951 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 01:50:47,951 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f4f63ba2b40>
2025-06-16 01:50:47,951 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 01:50:47,951 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 01:50:47,951 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 01:50:47,951 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 01:50:47,951 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 01:54:18,984 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 01:54:18 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 01:54:18,984 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 01:54:18,984 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 01:54:18,984 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 01:54:18,985 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 01:54:18,985 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 01:54:18,989 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 258, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 01:55:14,917 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:55:14,919 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:55:20,967 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 01:58:44,006 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:58:44,008 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 01:58:49,668 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 01:58:51,188 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 01:58:51,189 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x755b4e13b920>
2025-06-16 01:58:51,189 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 01:58:51,189 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 01:58:51,189 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 01:58:51,190 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 01:58:51,190 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 02:02:07,173 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 02:02:07 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 02:02:07,174 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 02:02:07,174 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 02:02:07,174 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 02:02:07,174 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 02:02:07,174 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 02:02:07,236 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.060s]
2025-06-16 02:12:07,179 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:12:07,181 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:12:12,223 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 02:13:59,141 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:13:59,144 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:14:04,734 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 02:14:07,723 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 02:14:07,724 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7b265519dbe0>
2025-06-16 02:14:07,724 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 02:14:07,724 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 02:14:07,724 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 02:14:07,724 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 02:14:07,724 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 02:17:11,196 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 02:17:11 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 02:17:11,196 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 02:17:11,196 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 02:17:11,196 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 02:17:11,197 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 02:17:11,197 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 02:17:11,203 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 257, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 02:18:17,996 - httpcore.connection - DEBUG - close.started
2025-06-16 02:18:17,996 - httpcore.connection - DEBUG - close.complete
2025-06-16 02:18:17,996 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 02:18:17,996 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7b265510fbc0>
2025-06-16 02:18:17,996 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 02:18:17,997 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 02:18:17,997 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 02:18:17,997 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 02:18:17,997 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 02:21:53,709 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 02:21:53 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 02:21:53,709 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 02:21:53,709 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 02:21:53,709 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 02:21:53,709 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 02:21:53,709 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 02:21:53,713 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 257, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 02:22:21,223 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:22:21,226 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:22:36,296 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 02:22:37,258 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 02:22:37,259 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74aab459a570>
2025-06-16 02:22:37,259 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 02:22:37,259 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 02:22:37,259 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 02:22:37,259 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 02:22:37,259 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 02:25:35,701 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 02:25:35 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 02:25:35,701 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 02:25:35,701 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 02:25:35,701 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 02:25:35,702 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 02:25:35,702 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 02:25:35,708 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 251, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 02:25:56,618 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:25:56,621 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:26:01,602 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 02:26:02,738 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 02:26:02,738 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x706f403aa720>
2025-06-16 02:26:02,739 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 02:26:02,739 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 02:26:02,739 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 02:26:02,739 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 02:26:02,739 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 02:29:18,727 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 02:29:18 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 02:29:18,728 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 02:29:18,728 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 02:29:18,729 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 02:29:18,729 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 02:29:18,729 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 02:29:19,013 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.282s]
2025-06-16 02:30:25,230 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:30:25,233 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 02:30:30,305 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 02:30:37,156 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 02:30:37,157 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7be96076a570>
2025-06-16 02:30:37,158 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 02:30:37,158 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 02:30:37,158 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 02:30:37,158 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 02:30:37,159 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 02:33:35,222 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 02:33:35 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 02:33:35,223 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 02:33:35,223 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 02:33:35,223 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 02:33:35,223 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 02:33:35,224 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 02:33:35,229 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 251, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 03:54:21,464 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 03:54:21,467 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 03:54:26,999 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 03:54:32,006 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 03:54:32,007 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7ccc031a59d0>
2025-06-16 03:54:32,007 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 03:54:32,007 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 03:54:32,007 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 03:54:32,007 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 03:54:32,007 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 03:59:08,327 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 03:59:08 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 03:59:08,327 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 03:59:08,327 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 03:59:08,328 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 03:59:08,328 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 03:59:08,328 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 03:59:08,597 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.267s]
2025-06-16 03:59:39,105 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 03:59:39,108 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 03:59:44,525 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:02:09,353 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:02:09,353 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x71422e1aa6f0>
2025-06-16 04:02:09,353 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:02:09,353 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:02:09,353 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:02:09,353 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:02:09,353 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 04:03:32,161 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 04:03:32 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 04:03:32,162 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 04:03:32,162 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 04:03:32,162 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 04:03:32,162 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 04:03:32,163 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-16 04:03:32,176 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: POST predict: Post "http://127.0.0.1:39967/completion": EOF (status code: -1)
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 251, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: POST predict: Post "http://127.0.0.1:39967/completion": EOF (status code: -1)
2025-06-16 04:03:33,271 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:03:33,273 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:03:38,416 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:03:40,829 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:03:40,829 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x756879f923c0>
2025-06-16 04:03:40,830 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:03:40,830 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:03:40,830 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:03:40,830 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:03:40,830 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 04:06:49,612 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 04:06:49 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 04:06:49,613 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 04:06:49,613 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 04:06:49,613 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 04:06:49,613 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 04:06:49,613 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 04:06:49,620 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 251, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 04:08:31,052 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:08:31,054 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:08:36,194 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:08:36,704 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:08:36,707 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:08:43,074 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:08:43,918 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:08:43,919 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7c20343a27e0>
2025-06-16 04:08:43,919 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:08:43,920 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:08:43,920 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:08:43,920 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:08:43,920 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 04:11:43,270 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 04:11:43 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 04:11:43,271 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 04:11:43,271 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 04:11:43,271 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 04:11:43,271 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 04:11:43,271 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 04:11:43,277 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 251, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 04:12:00,618 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:12:00,621 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:12:05,552 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:12:06,700 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:12:06,701 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7dfeab182450>
2025-06-16 04:12:06,701 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:12:06,701 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:12:06,701 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:12:06,701 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:12:06,701 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 04:15:23,284 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 04:15:23 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 04:15:23,285 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 04:15:23,285 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 04:15:23,286 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 04:15:23,286 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 04:15:23,286 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 04:15:23,833 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.545s]
2025-06-16 04:15:28,235 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:15:28,237 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:15:39,174 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:16:46,127 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:16:46,128 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x709bef3dbf50>
2025-06-16 04:16:46,128 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:16:46,128 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:16:46,129 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:16:46,129 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:16:46,129 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 04:21:13,590 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 04:21:13 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 04:21:13,590 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 04:21:13,590 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 04:21:13,591 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 04:21:13,591 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 04:21:13,591 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 04:21:14,026 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.432s]
2025-06-16 04:21:25,531 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:21:25,534 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:21:33,041 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:26:37,981 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:26:37,984 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:26:43,052 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:26:44,208 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:26:44,209 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7156d0b96360>
2025-06-16 04:26:44,209 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:26:44,209 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:26:44,209 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:26:44,209 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:26:44,209 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 04:32:39,227 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 04:32:39 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 04:32:39,228 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 04:32:39,228 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 04:32:39,229 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 04:32:39,229 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 04:32:39,229 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 04:32:39,279 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.047s]
2025-06-16 04:32:40,351 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:32:40,353 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:32:45,332 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:36:44,599 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:36:44,601 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:36:50,951 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:36:52,089 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:36:52,089 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7bb93a365a60>
2025-06-16 04:36:52,090 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:36:52,090 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:36:52,090 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:36:52,090 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:36:52,090 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 04:41:18,519 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 04:41:18 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 04:41:18,520 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 04:41:18,520 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 04:41:18,521 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 04:41:18,521 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 04:41:18,521 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 04:41:18,761 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.237s]
2025-06-16 04:45:10,204 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:45:10,206 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:45:15,213 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:45:34,765 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:45:34,768 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:45:39,790 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:45:39,798 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:45:39,799 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7bef72b723f0>
2025-06-16 04:45:39,799 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:45:39,799 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:45:39,799 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:45:39,799 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:45:39,799 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 04:48:38,525 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 04:48:38 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 04:48:38,526 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 04:48:38,526 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 04:48:38,526 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 04:48:38,526 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 04:48:38,526 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 04:48:38,533 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 271, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 04:49:42,250 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:49:42,252 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:49:49,672 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:50:17,545 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:50:17,545 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7c32db96e2d0>
2025-06-16 04:50:17,546 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:50:17,546 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:50:17,546 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:50:17,546 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:50:17,546 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 04:53:15,591 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 04:53:15 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 04:53:15,592 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 04:53:15,592 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 04:53:15,592 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 04:53:15,592 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 04:53:15,592 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 04:53:15,599 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 265, in chat_endpoint
    "Step 4: Therefore, the sequence is [recovered sequence]."
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 04:53:16,596 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:53:16,599 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:53:21,802 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:56:57,463 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:56:57,466 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:57:02,602 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:57:40,916 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:57:40,919 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 04:57:45,896 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 04:57:51,104 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 04:57:51,105 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72cb56e6e360>
2025-06-16 04:57:51,105 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 04:57:51,105 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 04:57:51,105 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 04:57:51,105 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 04:57:51,105 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 05:00:43,807 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 05:00:43 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 05:00:43,808 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 05:00:43,808 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 05:00:43,809 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 05:00:43,809 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 05:00:43,809 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 05:00:43,876 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.064s]
2025-06-16 05:09:55,474 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:09:55,476 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:10:00,506 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:10:03,002 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 05:10:03,003 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7b115419e270>
2025-06-16 05:10:03,003 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 05:10:03,003 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 05:10:03,003 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 05:10:03,003 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 05:10:03,004 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 05:13:07,313 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 05:13:07 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 05:13:07,314 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 05:13:07,314 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 05:13:07,314 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 05:13:07,314 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 05:13:07,314 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 05:13:07,320 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: No generations found in stream.
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 260, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: No generations found in stream.
2025-06-16 05:13:26,658 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:13:26,661 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:13:31,659 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.044s]
2025-06-16 05:15:24,638 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:15:24,641 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:15:30,148 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:15:45,008 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 05:15:45,009 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x77fcff0b6690>
2025-06-16 05:15:45,009 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 05:15:45,009 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 05:15:45,009 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 05:15:45,009 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 05:15:45,010 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 05:21:39,270 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 05:21:39 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 05:21:39,271 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 05:21:39,271 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 05:21:39,272 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 05:21:39,272 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 05:21:39,272 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 05:21:39,326 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.052s]
2025-06-16 05:23:40,997 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:23:40,999 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:23:47,073 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:24:07,912 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:24:07,915 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:24:12,974 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:24:15,290 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 05:24:15,291 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x70d01d7823c0>
2025-06-16 05:24:15,291 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 05:24:15,291 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 05:24:15,291 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 05:24:15,291 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 05:24:15,291 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 05:27:11,460 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 05:27:11 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 05:27:11,460 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 05:27:11,460 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 05:27:11,461 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 05:27:11,461 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 05:27:11,461 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 05:27:11,507 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.043s]
2025-06-16 05:27:55,634 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:27:55,636 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:28:00,592 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:28:04,702 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:28:04,704 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:28:09,744 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:29:07,795 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:29:07,797 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:29:13,658 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:29:17,750 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 05:29:17,751 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x746f25f769c0>
2025-06-16 05:29:17,752 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 05:29:17,752 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 05:29:17,752 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 05:29:17,752 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 05:29:17,752 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 05:34:25,351 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 05:34:25 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 05:34:25,351 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 05:34:25,351 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 05:34:25,352 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 05:34:25,352 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 05:34:25,352 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 05:34:25,980 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.624s]
2025-06-16 05:42:46,310 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:42:46,312 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:42:51,358 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:42:53,286 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 05:42:53,287 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7b499bd66450>
2025-06-16 05:42:53,287 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 05:42:53,288 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 05:42:53,288 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 05:42:53,288 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 05:42:53,288 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 05:47:37,501 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 05:47:37 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 05:47:37,502 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 05:47:37,502 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 05:47:37,503 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 05:47:37,503 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 05:47:37,503 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 05:47:37,589 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.083s]
2025-06-16 05:47:40,313 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:47:40,316 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:47:46,059 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:48:55,972 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:48:55,974 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:49:01,729 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:49:02,264 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:49:02,267 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:49:09,445 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:52:58,810 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:52:58,813 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:53:06,377 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:56:23,999 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:56:24,002 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 05:56:29,792 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 05:56:31,079 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 05:56:31,080 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7252d5f85af0>
2025-06-16 05:56:31,080 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 05:56:31,080 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 05:56:31,080 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 05:56:31,080 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 05:56:31,080 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 06:01:25,201 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 06:01:25 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 06:01:25,202 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 06:01:25,202 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 06:01:25,203 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 06:01:25,203 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 06:01:25,203 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 06:01:25,504 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.298s]
2025-06-16 06:08:17,474 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:08:17,476 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:08:25,200 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 06:08:41,527 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 06:08:41,528 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e86e4977ce0>
2025-06-16 06:08:41,529 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 06:08:41,529 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 06:08:41,529 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 06:08:41,529 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 06:08:41,529 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 06:13:09,327 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 06:13:09 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 06:13:09,327 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 06:13:09,328 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 06:13:09,328 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 06:13:09,328 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 06:13:09,329 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 06:13:09,424 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.092s]
2025-06-16 06:13:10,619 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:13:10,621 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:13:16,090 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 06:13:55,314 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:13:55,316 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:14:03,016 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 06:14:23,758 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:14:23,760 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:14:29,602 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.045s]
2025-06-16 06:14:32,391 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 06:14:32,391 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75e6561a65a0>
2025-06-16 06:14:32,391 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 06:14:32,392 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 06:14:32,392 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 06:14:32,392 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 06:14:32,392 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 06:18:52,700 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 06:18:52 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 06:18:52,700 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 06:18:52,700 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 06:18:52,701 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 06:18:52,701 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 06:18:52,701 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 06:18:53,602 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.899s]
2025-06-16 06:18:54,935 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:18:54,938 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:19:02,859 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 06:29:09,625 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:29:09,628 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-16 06:29:15,588 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-16 06:29:21,404 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 06:29:21,405 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x703e489a6660>
2025-06-16 06:29:21,405 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 06:29:21,406 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 06:29:21,406 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 06:29:21,406 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 06:29:21,406 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 06:34:10,407 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 06:34:10 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 06:34:10,408 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 06:34:10,408 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 06:34:10,409 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 06:34:10,409 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 06:34:10,409 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 06:34:10,705 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.294s]
2025-06-16 06:37:21,192 - httpcore.connection - DEBUG - close.started
2025-06-16 06:37:21,193 - httpcore.connection - DEBUG - close.complete
2025-06-16 06:37:21,193 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-16 06:37:21,193 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x703e489a6000>
2025-06-16 06:37:21,193 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-16 06:37:21,193 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-16 06:37:21,193 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-16 06:37:21,193 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-16 06:37:21,193 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-16 06:41:53,451 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Mon, 16 Jun 2025 06:41:53 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-16 06:41:53,452 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-16 06:41:53,452 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-16 06:41:53,453 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-16 06:41:53,453 - httpcore.http11 - DEBUG - response_closed.started
2025-06-16 06:41:53,453 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-16 06:41:53,741 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.286s]
2025-06-17 00:19:45,521 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:19:45,524 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:19:51,182 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.049s]
2025-06-17 00:19:59,072 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:19:59,074 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:20:04,533 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 00:20:13,235 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 00:20:13,236 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f492d5aa3f0>
2025-06-17 00:20:13,236 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 00:20:13,236 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 00:20:13,236 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 00:20:13,237 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 00:20:13,237 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 00:23:50,970 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 00:23:50 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 00:23:50,971 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 00:23:50,971 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 00:23:50,972 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 00:23:50,972 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 00:23:50,972 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 00:23:51,290 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.316s]
2025-06-17 00:24:37,669 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:24:37,672 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:24:42,762 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.002s]
2025-06-17 00:24:52,721 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 00:24:52,722 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x73f9e25a65d0>
2025-06-17 00:24:52,722 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 00:24:52,723 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 00:24:52,723 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 00:24:52,723 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 00:24:52,723 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 00:28:06,354 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 00:28:06 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 00:28:06,354 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 00:28:06,354 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 00:28:06,355 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 00:28:06,355 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 00:28:06,355 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 00:28:07,233 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.876s]
2025-06-17 00:31:40,987 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:31:40,989 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:31:48,560 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.009s]
2025-06-17 00:32:20,126 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 00:32:20,126 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7ec69b15fb00>
2025-06-17 00:32:20,126 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 00:32:20,127 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 00:32:20,127 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 00:32:20,127 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 00:32:20,127 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 00:35:34,412 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 00:35:34 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 00:35:34,412 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 00:35:34,413 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 00:35:34,413 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 00:35:34,413 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 00:35:34,413 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 00:35:34,511 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.096s]
2025-06-17 00:36:18,752 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:36:18,755 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:36:24,099 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.009s]
2025-06-17 00:36:29,669 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:36:29,672 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:36:34,616 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 00:36:35,294 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 00:36:35,294 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75c76208fb00>
2025-06-17 00:36:35,295 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 00:36:35,295 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 00:36:35,295 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 00:36:35,295 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 00:36:35,295 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 00:39:49,264 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 00:39:49 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 00:39:49,264 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 00:39:49,265 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 00:39:49,265 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 00:39:49,266 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 00:39:49,266 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 00:39:49,363 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.095s]
2025-06-17 00:48:42,769 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:48:42,772 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:48:47,995 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.009s]
2025-06-17 00:48:48,003 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 00:48:48,003 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e819cdadca0>
2025-06-17 00:48:48,003 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 00:48:48,003 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 00:48:48,003 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 00:48:48,003 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 00:48:48,003 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 00:51:43,197 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 00:51:43 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 00:51:43,205 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 00:51:43,206 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 00:51:43,209 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 00:51:43,210 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 00:51:43,216 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 00:51:43,232 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: POST predict: Post "http://127.0.0.1:43931/completion": EOF (status code: -1)
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 241, in chat_endpoint
    start_time = time.time()
                 ^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 226, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: POST predict: Post "http://127.0.0.1:43931/completion": EOF (status code: -1)
2025-06-17 00:51:47,935 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:51:47,938 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:51:53,198 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 00:52:04,921 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:52:04,924 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:52:12,357 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 00:52:13,474 - api.api_server - ERROR - 오류 발생: '\n    "ambiguous_command"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 258, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n    "ambiguous_command"'
2025-06-17 00:53:12,235 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:53:12,237 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 00:53:17,217 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 00:53:17,232 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 00:53:17,232 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x78a0ccf9ecc0>
2025-06-17 00:53:17,232 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 00:53:17,233 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 00:53:17,233 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 00:53:17,233 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 00:53:17,233 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 00:58:01,685 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 00:58:01 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 00:58:01,685 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 00:58:01,686 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 00:58:01,686 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 00:58:01,686 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 00:58:01,686 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 00:58:01,989 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.300s]
2025-06-17 01:04:28,918 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:04:28,921 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:04:34,088 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 01:04:36,635 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 01:04:36,636 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x751345986630>
2025-06-17 01:04:36,636 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 01:04:36,636 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 01:04:36,636 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 01:04:36,636 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 01:04:36,637 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 01:07:13,956 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 01:07:13 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 01:07:13,986 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 01:07:13,995 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 01:07:14,030 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 01:07:14,033 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 01:07:14,034 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 01:07:14,263 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.162s]
2025-06-17 01:11:14,701 - httpcore.connection - DEBUG - close.started
2025-06-17 01:11:14,702 - httpcore.connection - DEBUG - close.complete
2025-06-17 01:11:14,702 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 01:11:14,705 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7513476fa6c0>
2025-06-17 01:11:14,705 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 01:11:14,706 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 01:11:14,706 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 01:11:14,706 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 01:11:14,706 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 01:12:11,123 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 01:12:11 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 01:12:11,124 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 01:12:11,124 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 01:12:11,131 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 01:12:11,132 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 01:12:11,132 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 01:12:12,001 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.866s]
2025-06-17 01:12:13,181 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:12:13,183 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:12:18,441 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.002s]
2025-06-17 01:15:51,699 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 01:15:51,700 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7bcab036e960>
2025-06-17 01:15:51,700 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 01:15:51,701 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 01:15:51,701 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 01:15:51,701 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 01:15:51,701 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 01:18:42,763 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 01:18:42 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 01:18:42,763 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 01:18:42,763 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 01:18:42,764 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 01:18:42,764 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 01:18:42,764 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 01:18:43,160 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.392s]
2025-06-17 01:43:28,341 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:43:28,344 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:43:36,773 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.053s]
2025-06-17 01:43:37,673 - api.api_server - ERROR - 오류 발생: '\n    "ambiguous_command"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 258, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n    "ambiguous_command"'
2025-06-17 01:43:55,393 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:43:55,395 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:44:02,284 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 01:44:02,285 - api.api_server - ERROR - 오류 발생: '\n    "ambiguous_command"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 254, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n    "ambiguous_command"'
2025-06-17 01:44:27,951 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:44:27,954 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 01:44:33,151 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 01:44:36,647 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 01:44:36,648 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7063abb6e8d0>
2025-06-17 01:44:36,648 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 01:44:36,649 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 01:44:36,649 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 01:44:36,649 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 01:44:36,649 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 01:49:05,239 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 01:49:05 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 01:49:05,240 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 01:49:05,240 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 01:49:05,241 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 01:49:05,241 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 01:49:05,241 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 01:49:05,432 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.188s]
2025-06-17 02:13:39,774 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 02:13:39,777 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 02:13:44,852 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.055s]
2025-06-17 02:13:59,254 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 02:13:59,257 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 02:14:05,417 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 02:14:07,751 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 02:14:07,752 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7c65852ba930>
2025-06-17 02:14:07,752 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 02:14:07,752 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 02:14:07,752 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 02:14:07,752 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 02:14:07,752 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 02:19:53,457 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 02:19:53 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 02:19:53,465 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 02:19:53,465 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 02:19:53,466 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 02:19:53,466 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 02:19:53,466 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 02:19:53,577 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.108s]
2025-06-17 02:28:44,586 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 02:28:44,588 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 02:28:53,294 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 02:29:03,389 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 02:29:03,390 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7bac46d6a9c0>
2025-06-17 02:29:03,391 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 02:29:03,391 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 02:29:03,391 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 02:29:03,392 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 02:29:03,392 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 02:34:27,586 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 02:34:27 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 02:34:27,595 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 02:34:27,596 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 02:34:27,596 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 02:34:27,596 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 02:34:27,597 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 02:34:27,758 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.158s]
2025-06-17 04:18:53,580 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 04:18:53,583 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 04:18:58,845 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 04:19:00,643 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 04:19:00,643 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x79ffdc9669c0>
2025-06-17 04:19:00,644 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 04:19:00,644 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 04:19:00,644 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 04:19:00,644 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 04:19:00,644 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 04:24:39,870 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 04:24:39 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 04:24:39,877 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 04:24:39,878 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 04:24:39,879 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 04:24:39,879 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 04:24:39,879 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 04:24:39,991 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.109s]
2025-06-17 04:42:24,796 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 04:42:24,799 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 04:42:40,974 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 04:42:40,982 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 04:42:40,982 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f41f0d6aae0>
2025-06-17 04:42:40,982 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 04:42:40,983 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 04:42:40,983 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 04:42:40,983 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 04:42:40,983 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 04:47:57,928 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 04:47:57 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 04:47:57,937 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 04:47:57,937 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 04:47:57,938 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 04:47:57,938 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 04:47:57,938 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 04:47:58,056 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.115s]
2025-06-17 05:40:08,313 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:40:08,316 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:40:14,261 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.044s]
2025-06-17 05:40:16,394 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-06-17 05:40:16,394 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x764f15341850>
2025-06-17 05:40:16,394 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 05:40:16,395 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 05:40:16,395 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 05:40:16,395 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 05:40:16,395 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 05:46:17,946 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 17 Jun 2025 05:46:17 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-06-17 05:46:17,955 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-06-17 05:46:17,955 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 05:46:17,956 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 05:46:17,956 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 05:46:17,956 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 05:46:18,073 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.114s]
2025-06-17 05:53:40,255 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:53:40,858 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:53:40,868 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:53:46,430 - api.api_server - ERROR - 오류 발생: name 'OllamaAgentManager' is not defined
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 94, in startup_event
    agent_manager = OllamaAgentManager(mcp_client, tools)
                    ^^^^^^^^^^^^^^^^^^
NameError: name 'OllamaAgentManager' is not defined. Did you mean: 'MistralAgentManager'?
2025-06-17 05:53:55,498 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:53:55,501 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:54:02,149 - api.api_server - ERROR - 오류 발생: 1 validation error for Security
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x726b87236180>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    mistral_llm = Mistral(api_key=self.api_key)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/mistralai/sdk.py", line 99, in __init__
    security = models.Security(api_key=api_key)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for Security
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x726b87236180>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 05:55:43,878 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:55:44,481 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:55:44,484 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:55:50,959 - api.api_server - ERROR - 오류 발생: 'Mistral' object has no attribute 'bind_tools'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 57, in createChatModel
    self.agent = create_react_agent(
                 ^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py", line 160, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py", line 691, in create_react_agent
    model = cast(BaseChatModel, model).bind_tools(tool_classes)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Mistral' object has no attribute 'bind_tools'
2025-06-17 05:55:51,348 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:57:14,637 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:57:15,095 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:57:15,097 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:57:22,682 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x769ecdfafda0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x769ecd504040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x769ecdfafda0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x769ecd504040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 05:57:33,277 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:57:33,280 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:57:38,372 - api.api_server - ERROR - 오류 발생: name 't3sHXJz0Jf1t3JY6l5f3JPsQU9oCXu8W' is not defined
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 54, in createChatModel
    api_key="t3sHXJz0Jf1t3JY6l5f3JPsQU9oCXu8W",
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
NameError: name 't3sHXJz0Jf1t3JY6l5f3JPsQU9oCXu8W' is not defined
2025-06-17 05:57:39,923 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:57:39,926 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:57:44,741 - api.api_server - ERROR - 오류 발생: 1 validation error for ChatMistralAI
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x759817870040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for ChatMistralAI
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x759817870040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 05:57:58,809 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:57:58,811 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:58:05,204 - api.api_server - ERROR - 오류 발생: 1 validation error for ChatMistralAI
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x71b9595abe20>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for ChatMistralAI
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x71b9595abe20>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 05:58:15,112 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:58:15,114 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:58:19,988 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x7bd9f306b7a0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x7bd9f2500040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x7bd9f306b7a0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x7bd9f2500040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 05:58:47,468 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:58:47,471 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:58:52,563 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x7e8f5657d820>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x7e8f55b04040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    api_key=self.api_key,
              ^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x7e8f5657d820>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x7e8f55b04040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 05:58:53,382 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:58:53,384 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:58:59,937 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x7358635ada30>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x735863634040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 52, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x7358635ada30>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x735863634040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 05:59:13,059 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:59:13,061 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:59:17,921 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x742784c659a0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x742784af0040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 52, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x742784c659a0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x742784af0040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 05:59:20,058 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:59:20,060 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 05:59:25,349 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x786cc1ab98e0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x786cc0f24040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x786cc1ab98e0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x786cc0f24040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 06:00:29,092 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:00:29,094 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:00:34,127 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x77d052e61a90>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x77d052cec040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x77d052e61a90>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x77d052cec040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 06:07:29,627 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:07:29,629 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:07:37,400 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x761b6e58da30>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x761b6db14040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x761b6e58da30>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x761b6db14040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 06:07:52,332 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:07:52,335 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:07:57,741 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x7be754b65a30>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x7be754108040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 53, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x7be754b65a30>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x7be754108040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 06:10:00,396 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:10:00,399 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:10:06,559 - api.api_server - ERROR - 오류 발생: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x763be6fe9ca0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x763be7070040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager.createChatModel(
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 52, in createChatModel
    mistral_llm = ChatMistralAI(
                  ^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/.venv/lib/python3.12/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatMistralAI
api_key
  Input should be a valid string [type=string_type, input_value=<langchain_mcp_adapters.c...bject at 0x763be6fe9ca0>, input_type=MultiServerMCPClient]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
model
  Input should be a valid string [type=string_type, input_value=[StructuredTool(name='get...ool at 0x763be7070040>)], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-17 06:10:15,275 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:10:15,278 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:10:20,302 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.045s]
2025-06-17 06:10:23,366 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:10:23,410 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e89e711d6d0>
2025-06-17 06:10:23,410 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7e89e9a5d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:10:23,419 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e89e711d610>
2025-06-17 06:10:23,419 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:10:23,420 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:10:23,420 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:10:23,420 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:10:23,420 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:10:26,287 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:10:26 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'2567'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2568'), (b'x-kong-proxy-latency', b'17'), (b'x-kong-request-id', b'd91c06561a75cd4a6eca364431735101'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=6ebrBqND1l87rZnm34odSP86dVo5YL7LO_KdEdpeZww-1750140626-1.0.1.1-ngemes.xDdwfXa1xltV7T05OqrnEpyDSZLspBkKhrgV7ZkbvJCYosaHab_NnkVqOaZG7NMkqNnv39prG7Wr5v.8LS_rxrVC_hIhs6wvWokk; path=/; expires=Tue, 17-Jun-25 06:40:26 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=zZk7HKY.3uRjFovpcUTtZ_2o.jc0_vueDyNyseZNGa0-1750140626277-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951062305dcac102-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:10:26,288 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:10:26,288 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:10:38,593 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:10:38,593 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:10:38,594 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:10:38,666 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.069s]
2025-06-17 06:12:25,177 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:12:25,180 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:12:30,496 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:12:32,201 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:12:32,205 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x704bd23b8a40>
2025-06-17 06:12:32,205 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x704bd36a56d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:12:32,212 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x704bd203f290>
2025-06-17 06:12:32,212 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:12:32,212 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:12:32,212 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:12:32,213 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:12:32,213 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:12:34,862 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:12:34 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'2350'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2352'), (b'x-kong-proxy-latency', b'18'), (b'x-kong-request-id', b'088e3a01063a633ac3d9db4ae01ae100'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=DNwmY71py39DzsyLD6PFYsNRIVV_yHArs_iA8RtfaWA-1750140754-1.0.1.1-MDDSppXH30nJ3vy_lapgL8mFiSBPC64NoJyrTll7.Vhf2_YtQUoijSX6R8c3CymjFWUFt54OZ3d4N0fVIh5i4biEBBvBwyI7R4wWn93s2js; path=/; expires=Tue, 17-Jun-25 06:42:34 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=zBvRklmGbIDDBFBuOQIdJ3AwLB_CywCOLvWawqWvt90-1750140754854-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951065554ceeea97-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:12:34,862 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:12:34,862 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:12:46,256 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:12:46,256 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:12:46,260 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:12:46,325 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.057s]
2025-06-17 06:16:53,896 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:16:53,898 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:16:59,054 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.009s]
2025-06-17 06:17:00,485 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:17:00,549 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72e00f33d8b0>
2025-06-17 06:17:00,549 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72e0113c16d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:17:00,561 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72e010250cb0>
2025-06-17 06:17:00,562 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:17:00,562 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:17:00,562 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:17:00,563 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:17:00,563 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:17:01,261 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:17:01 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'413'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'414'), (b'x-kong-proxy-latency', b'18'), (b'x-kong-request-id', b'66d02b272225637054adb65903989934'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=DEBf12YYEe9xYMMHfGyhd08GjEyb2g9nizOVCxrN5P4-1750141021-1.0.1.1-Kvq2P_WBEbYseuINHmjfykNUipD1gEvtQHFflKbLogkb7CwXoVLO6eABdhaMetg7e2qk9cP9HVTfpceOCcoXrlt.hxV7CP.bZTSDT2iV4SA; path=/; expires=Tue, 17-Jun-25 06:47:01 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=iMtekuQMIx4YqS6R30y63_q.GsCSAhBSixe7Zs8WTlI-1750141021252-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95106be27fadea99-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:17:01,262 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:17:01,262 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:17:10,297 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:17:10,298 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:17:10,301 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:17:10,368 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.059s]
2025-06-17 06:18:45,082 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:18:45,085 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:18:49,741 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:18:51,074 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:18:51,079 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7af71e509970>
2025-06-17 06:18:51,079 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7af7205d16d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:18:51,090 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7af71e5098b0>
2025-06-17 06:18:51,091 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:18:51,091 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:18:51,091 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:18:51,091 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:18:51,091 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:18:51,755 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:18:51 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'378'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'378'), (b'x-kong-proxy-latency', b'17'), (b'x-kong-request-id', b'd8846f1c6d854323c881150db320083a'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=oU5Mxz5cRi8A1yi2AQXfYH3mIU75UWYe8po_FVoKajA-1750141131-1.0.1.1-uuqmgtLPsFNT1FEzO_c6xP593gMEiYF02sBhRelWopHD75KBbShDa744aRFQ_sPJiT61EqjW5Iq4rnegkCSaiEGCOgXGD2ApYXwxJaFORJ0; path=/; expires=Tue, 17-Jun-25 06:48:51 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=DNyTJO7LQGz.dP7W8fPVCtVmVSdsGVCTLSSEgqpCXCE-1750141131743-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95106e954d6a8b5b-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:18:51,756 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:18:51,757 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:19:01,132 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:19:01,133 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:19:01,136 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:19:01,212 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.070s]
2025-06-17 06:22:25,114 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:22:25,160 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7af71f8b95e0>
2025-06-17 06:22:25,160 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7af7205d16d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:22:25,169 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7af71ef0c920>
2025-06-17 06:22:25,169 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:22:25,169 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:22:25,169 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:22:25,169 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:22:25,169 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:22:26,139 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:22:26 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'623'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'624'), (b'x-kong-proxy-latency', b'42'), (b'x-kong-request-id', b'6c6b126a1aa0747d10271663ed525568'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951073cf4ba3aa7d-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:22:26,140 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:22:26,140 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:22:37,022 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:22:37,022 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:22:37,023 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:22:37,083 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.058s]
2025-06-17 06:24:08,803 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:24:08,808 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7af71efd54f0>
2025-06-17 06:24:08,808 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7af7205d16d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:24:08,821 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7af71efd6030>
2025-06-17 06:24:08,821 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:24:08,821 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:24:08,822 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:24:08,822 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:24:08,822 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:24:10,798 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:24:10 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1672'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1672'), (b'x-kong-proxy-latency', b'39'), (b'x-kong-request-id', b'18c9916a7c54992241a70e4c613ad862'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951076571d79e9fd-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:24:10,798 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:24:10,798 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:24:24,942 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:24:24,942 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:24:24,943 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:24:24,998 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.053s]
2025-06-17 06:40:55,917 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:40:55,920 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:41:03,749 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.009s]
2025-06-17 06:41:03,756 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:41:03,802 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7d3f7af24b00>
2025-06-17 06:41:03,803 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7d3f7cfcd6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:41:03,815 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7d3f7af25a00>
2025-06-17 06:41:03,816 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:41:03,816 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:41:03,817 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:41:03,817 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:41:03,817 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:41:05,685 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:41:05 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1599'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1600'), (b'x-kong-proxy-latency', b'8'), (b'x-kong-request-id', b'e191be0615602731a81c0937441c922f'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=fXo9peNV7vAoxL31TqE.H_.e1PL9YyRZaip6XqGI_gU-1750142465-1.0.1.1-2D0y8xWDTy7.lifRNTkOkhiCh_mFJvUGPi3Cism3SG9GVs_3XKe63WlaZATq0CLhF4hW7NkKpGRq4838V_54ZpLtuTgGXWFTniqNKtbX.ow; path=/; expires=Tue, 17-Jun-25 07:11:05 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=OjD12kyio5Wggz_SMBOxOZnZ0qrhvbenP.xKDuIxfp8-1750142465688-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95108f1eecf7ea31-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:41:05,686 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:41:05,692 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:41:14,290 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:41:14,291 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:41:14,294 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:41:15,178 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.876s]
2025-06-17 06:44:36,888 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:44:36,890 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:44:41,881 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:44:44,015 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:44:44,018 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:44:50,834 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:44:52,964 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:44:52,969 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7906df90d8e0>
2025-06-17 06:44:52,969 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7906e10c16d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:44:52,981 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7906dff590a0>
2025-06-17 06:44:52,981 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:44:52,982 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:44:52,982 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:44:52,982 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:44:52,982 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:44:53,666 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:44:53 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'390'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'390'), (b'x-kong-proxy-latency', b'20'), (b'x-kong-request-id', b'6384b728594238414cfcd265742b7207'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=vxONCfLh7L95OsosYGfwyjHT6y2UzF2Vfha6QFceh9g-1750142693-1.0.1.1-8_AzK0tt2SR04hMHt8gNkhpL0zkAV6wDKuxkdriPOamFKzLyvyrtho7eLdrCzYGg_WwC.IWRA3x7lnBG7QG1SEQ9uQE5E51Z8uzQrb87xYo; path=/; expires=Tue, 17-Jun-25 07:14:53 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=XrNFY0Si7r5HmXNgXKrfBnzNQEOioePswDWeZJ8Y3ng-1750142693670-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951094b73efe8b5c-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:44:53,667 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:44:53,668 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:45:02,098 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:45:02,099 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:45:02,102 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:45:02,385 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.275s]
2025-06-17 06:47:41,297 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:47:41,299 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:47:46,561 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:47:47,712 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:47:47,715 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:47:53,744 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:48:10,805 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:48:10,857 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74bf91105b20>
2025-06-17 06:48:10,857 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x74bf93ad16d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:48:10,869 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74bf91105a60>
2025-06-17 06:48:10,869 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:48:10,870 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:48:10,870 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:48:10,870 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:48:10,870 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:48:13,678 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:48:13 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'2500'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2500'), (b'x-kong-proxy-latency', b'20'), (b'x-kong-request-id', b'ebd268611cc277489873fd1f510b7b7d'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=akcyIhH4611iJ13i1uOujXL7ELLPVDG74f8odz1meg4-1750142893-1.0.1.1-ASuz56LjaeLYdSFQKnpXGBn_vdWP28MhWGEcX8Mb7K.t8WMqqIlJb9c5GD8Iio1mdA__SzY6BlfsxkTxfakRV80L0OybcNfmCR0ThpGvXoE; path=/; expires=Tue, 17-Jun-25 07:18:13 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=4.Rkhd7bXq6HIMJwNjFaoHRH4MB59KI2bQCblHgIY8Q-1750142893684-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510998bf9f93115-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:48:13,680 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:48:13,680 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:48:22,315 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:48:22,315 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:48:22,318 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:48:22,402 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.077s]
2025-06-17 06:50:18,880 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:50:18,883 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:50:24,295 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:50:42,395 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:50:42,397 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:50:49,453 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:51:07,721 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:51:07,725 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7717b1331730>
2025-06-17 06:51:07,725 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7717b33c96d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:51:07,734 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7717b1e11d00>
2025-06-17 06:51:07,734 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:51:07,734 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:51:07,734 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:51:07,734 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:51:07,734 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:51:08,447 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:51:08 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'410'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'411'), (b'x-kong-proxy-latency', b'20'), (b'x-kong-request-id', b'2532e81a99134a615a1afc2a50aa255e'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=MUEkcG6K7okcBLLXHS0CLRcfkhpQbTFytaLG87aR5mU-1750143068-1.0.1.1-XvVqnl7USH40JAjoLRuqY_e.faeO1l4wFquhgyjIEmaYoRKRX3A0a30TOrYM73B5gP_CXZOw3HxQJO4km9gb3jRaAdNGgwMgqFSMsYmvdpw; path=/; expires=Tue, 17-Jun-25 07:21:08 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=S3P1SpZmiXZvjJUnae09QCMCty0PnV7E5UdnwhEyCZo-1750143068452-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95109ddd6de9aa4a-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:51:08,449 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:51:08,449 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:51:19,442 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:51:19,442 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:51:19,446 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:51:19,685 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.232s]
2025-06-17 06:53:40,412 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:53:40,414 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:53:45,200 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:54:05,120 - api.api_server - ERROR - 오류 발생: 'image_url'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 322, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: 'image_url'
2025-06-17 06:54:46,616 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:54:46,619 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:54:51,692 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:55:09,028 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:55:09,030 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:55:14,296 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:55:16,142 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:55:16,190 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x78e0fc818cb0>
2025-06-17 06:55:16,190 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x78e0fe8e56d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:55:16,201 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x78e0fc8199a0>
2025-06-17 06:55:16,202 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:55:16,202 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:55:16,203 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:55:16,203 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:55:16,203 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:55:16,894 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:55:16 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'396'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'396'), (b'x-kong-proxy-latency', b'21'), (b'x-kong-request-id', b'b9e7f076b4499476489be02bac9f682b'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=VfSIs5AK3IZbAWBLwO4A7hTXK64BX7dJ8O3g9WR0yps-1750143316-1.0.1.1-pF_GBvlh5.MfeIK9mw6dxp_ERUmgfNU9exMXKriDvdGHWYvoM9DpM.pvD7LLBEAIDBnRbFDpd9Z5bAr9BtQTqljnXdQ1LNpFXG7tnWDogCM; path=/; expires=Tue, 17-Jun-25 07:25:16 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=mjGdsiEs0rwfSDdNGaR__pQMYPeGxVzl.6_Tc046sf4-1750143316901-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510a3ee5a9b305e-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:55:16,895 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:55:16,896 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:55:27,475 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:55:27,475 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:55:27,476 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:55:28,269 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.791s]
2025-06-17 06:58:08,909 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:58:08,911 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:58:14,296 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:58:15,812 - api.api_server - ERROR - 오류 발생: '\n    "ambiguous_command"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 339, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n    "ambiguous_command"'
2025-06-17 06:58:59,048 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:58:59,050 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:59:07,679 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:59:08,943 - api.api_server - ERROR - 오류 발생: Single '}' encountered in format string
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 339, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
ValueError: Single '}' encountered in format string
2025-06-17 06:59:32,188 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:59:32,191 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 06:59:37,413 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 06:59:38,901 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:59:38,906 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c90cd40>
2025-06-17 06:59:38,906 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7cac6e95d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:59:38,917 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6d6a88c0>
2025-06-17 06:59:38,917 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:59:38,917 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:59:38,918 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:59:38,918 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:59:38,918 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:59:44,846 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:59:44 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'5651'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'5652'), (b'x-kong-proxy-latency', b'8'), (b'x-kong-request-id', b'c9776c5797d80a254a259d4e86b8576d'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=XsWRbOL9mn2ENbHAbSw534_FuGxLnVkqS_DkWiUzPr8-1750143584-1.0.1.1-PgZ1GLRC_OSisFU7Vuvfp3KOGgA..6v7JW4Og0R6.o3jkQxxkpXlvfOo0D4tDgddsNvzd0HeD3zgPr8RWtSP8WoSClL1SZZx173MIeEs2E4; path=/; expires=Tue, 17-Jun-25 07:29:44 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=u6r5hedEGIONkAyDr7OZyongybyIb9rHMPFVZM81Xoo-1750143584852-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510aa58494caa35-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:59:44,847 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:59:44,848 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:59:44,852 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:59:44,852 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:59:44,854 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:59:44,877 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:59:44,881 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c90f950>
2025-06-17 06:59:44,881 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7cac6e95d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:59:44,892 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c90cb00>
2025-06-17 06:59:44,892 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:59:44,893 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:59:44,893 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:59:44,893 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:59:44,893 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:59:45,856 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:59:45 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'658'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'658'), (b'x-kong-proxy-latency', b'26'), (b'x-kong-request-id', b'9f5d7302efb7e02116cd0f0f86b82654'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510aa7dab1cea92-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:59:45,857 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:59:45,857 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:59:45,860 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:59:45,861 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:59:45,863 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:59:45,886 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:59:45,891 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c945910>
2025-06-17 06:59:45,891 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7cac6e95d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:59:45,903 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c946d50>
2025-06-17 06:59:45,903 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:59:45,904 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:59:45,904 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:59:45,904 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:59:45,904 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:59:46,777 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:59:46 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'577'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'578'), (b'x-kong-proxy-latency', b'28'), (b'x-kong-request-id', b'3491af2ad9457a2f8496360f97961751'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510aa83fdc2aa65-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:59:46,778 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:59:46,778 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:59:46,782 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:59:46,782 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:59:46,785 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:59:46,798 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:59:46,801 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c90d400>
2025-06-17 06:59:46,801 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7cac6e95d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:59:46,808 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c9462a0>
2025-06-17 06:59:46,808 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:59:46,808 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:59:46,808 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:59:46,809 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:59:46,809 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:59:48,826 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:59:48 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1731'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1732'), (b'x-kong-proxy-latency', b'23'), (b'x-kong-request-id', b'b9165e79af6da3738a99a42a1e849200'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510aa89af84eaa7-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:59:48,826 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:59:48,827 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:59:49,250 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:59:49,251 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:59:49,253 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:59:49,284 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:59:49,289 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c944b30>
2025-06-17 06:59:49,289 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7cac6e95d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:59:49,300 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c944800>
2025-06-17 06:59:49,300 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:59:49,301 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:59:49,301 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:59:49,301 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:59:49,301 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:59:50,960 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:59:50 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1348'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1349'), (b'x-kong-proxy-latency', b'24'), (b'x-kong-request-id', b'adaeaf0bbde6ff2bdef96e0341f66f76'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510aa993834c446-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:59:50,960 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:59:50,960 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:59:50,961 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:59:50,961 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:59:50,961 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:59:50,967 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:59:50,970 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c96b440>
2025-06-17 06:59:50,970 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7cac6e95d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:59:50,978 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c969be0>
2025-06-17 06:59:50,978 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:59:50,978 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:59:50,978 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:59:50,978 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:59:50,978 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:59:52,067 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:59:52 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'805'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'806'), (b'x-kong-proxy-latency', b'4'), (b'x-kong-request-id', b'f8b7a838e487d10f50857921bbf80972'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510aaa3be0aa7d1-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:59:52,067 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:59:52,067 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:59:52,068 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:59:52,068 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:59:52,069 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:59:52,081 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 06:59:52,083 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c9908c0>
2025-06-17 06:59:52,084 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7cac6e95d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 06:59:52,093 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7cac6c9916d0>
2025-06-17 06:59:52,093 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 06:59:52,093 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 06:59:52,093 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 06:59:52,093 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 06:59:52,093 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 06:59:52,858 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 06:59:52 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'467'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'468'), (b'x-kong-proxy-latency', b'25'), (b'x-kong-request-id', b'79d4d8470fe6fa0a8839187c58106f05'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510aaaaa943326d-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 06:59:52,859 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 06:59:52,859 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 06:59:53,327 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 06:59:53,328 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 06:59:53,332 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 06:59:53,404 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.065s]
2025-06-17 07:01:07,915 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:01:07,917 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:01:15,342 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:01:19,038 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:01:19,040 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:01:24,267 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:02:32,575 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:02:32,624 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72b069910b90>
2025-06-17 07:02:32,625 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72b06c2d96d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:02:32,633 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72b069911850>
2025-06-17 07:02:32,633 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:02:32,633 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:02:32,633 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:02:32,634 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:02:32,634 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:02:33,675 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:02:33 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'740'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'741'), (b'x-kong-proxy-latency', b'22'), (b'x-kong-request-id', b'efa097289a61b820430925629dbc6f75'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=07ta0vGu8LDHRz1PjHd7DUssMpy2Fj66gwHS3RYY_AI-1750143753-1.0.1.1-p0Gj0Rl27G0Z9SUzdjC6ynGVIhWhyycwhKrSlOV3rBHYWasdomTu.LNVgDJXip8d2XlB_.8KfahFaLaciqkPuaS9Q43.skjWw1Gw8igK1Ko; path=/; expires=Tue, 17-Jun-25 07:32:33 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=84C4Jq6QwCEUlUudQK5etkDgGwzD91pQiJ4.tucEIdc-1750143753681-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510ae960a573296-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:02:33,676 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:02:33,677 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:02:40,844 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:02:40,844 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:02:40,845 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:02:41,401 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.555s]
2025-06-17 07:03:44,395 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:03:44,397 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:03:51,961 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:03:53,081 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:03:53,085 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72aa0ebaaa50>
2025-06-17 07:03:53,085 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72aa1019d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:03:53,092 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72aa0ebaa750>
2025-06-17 07:03:53,093 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:03:53,093 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:03:53,093 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:03:53,093 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:03:53,093 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:03:54,027 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:03:54 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'637'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'638'), (b'x-kong-proxy-latency', b'24'), (b'x-kong-request-id', b'afe47b5b105505438b9e672208d2496c'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=KFv6Qzsc7pQIjCbJYXcHMhRRoVsi_LZ9TCmh8I83oZw-1750143834-1.0.1.1-6.5VLJay2OEmMPOLdsuT4Cn1i3hXVjBVKA5U9jY.BPCEMlGQGusJLx3mxrCFctxq2RDoEaQpD.PVU433tGAZFvbOC.WZlAi7zVHPnjx2VYo; path=/; expires=Tue, 17-Jun-25 07:33:54 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=eaDD9rl3iCM4mA0e4kLCJQi5Dt4WQW4FdwBjFPyPC.4-1750143834034-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510b08cecd6d1dd-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:03:54,028 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:03:54,035 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:03:54,039 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:03:54,039 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:03:54,042 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:03:56,982 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:03:56,985 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72aa0e12b770>
2025-06-17 07:03:56,985 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72aa1019d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:03:56,993 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72aa0ecad6d0>
2025-06-17 07:03:56,993 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:03:56,993 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:03:56,993 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:03:56,993 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:03:56,993 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:03:59,995 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:04:00 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'2700'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2701'), (b'x-kong-proxy-latency', b'26'), (b'x-kong-request-id', b'f7130c75f079f872b4a502400126661e'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510b0a54d55ea92-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:03:59,995 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:03:59,996 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:03:59,998 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:03:59,999 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:04:00,001 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:04:01,796 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:04:01,802 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72aa0e161b80>
2025-06-17 07:04:01,803 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72aa1019d6d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:04:01,813 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72aa0e162990>
2025-06-17 07:04:01,814 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:04:01,814 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:04:01,814 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:04:01,815 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:04:01,815 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:04:02,232 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:04:02 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'118'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'118'), (b'x-kong-proxy-latency', b'27'), (b'x-kong-request-id', b'2752b30dfc6aab654b6f2a2d5f2ab95a'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510b0c36e67ea01-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:04:02,232 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:04:02,233 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:04:12,774 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:04:12,774 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:04:12,775 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:04:12,943 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.166s]
2025-06-17 07:06:56,340 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:06:56,342 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:07:01,371 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:07:02,198 - api.api_server - ERROR - 오류 발생: '\n    "ambiguous_command"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 320, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n    "ambiguous_command"'
2025-06-17 07:08:01,338 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:08:01,340 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:08:09,018 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:08:09,696 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:08:09,744 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x792164b0caa0>
2025-06-17 07:08:09,745 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x792166bd56d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:08:09,753 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7921655dac60>
2025-06-17 07:08:09,753 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:08:09,753 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:08:09,753 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:08:09,753 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:08:09,753 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:08:10,832 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:08:10 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'214'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'215'), (b'x-kong-proxy-latency', b'19'), (b'x-kong-request-id', b'e57da0077beff2704efbb97191373e6e'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=sZt7LswAUUlhEiu_JyJzIPX8Qkr_CB7yhL3oL6wIxlI-1750144090-1.0.1.1-ydaWOZV2vhabsM59hTOG7n95eui7PsYqnHwleX1GQWNmFYc89_J6qzJzjWvgJqAIph._uQuN5N6pHNGySxy7Q_AlBWarQ9ACMtZJFqPb8eQ; path=/; expires=Tue, 17-Jun-25 07:38:10 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=LiNH0pe1MZ8OBAV7TEjzB1RrR_sL9EO9sVHVROjLsiM-1750144090838-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510b6d109617b6d-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:08:10,832 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:08:10,832 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:08:22,468 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:08:22,468 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:08:22,472 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:08:22,760 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.281s]
2025-06-17 07:09:32,165 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:09:32,167 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:09:39,338 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:10:10,899 - api.api_server - ERROR - 오류 발생: Invalid query format: Explicit command not found
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 301, in chat_endpoint
    raise ValueError("Invalid query format: Explicit command not found")
ValueError: Invalid query format: Explicit command not found
2025-06-17 07:10:26,742 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:10:26,744 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:10:31,842 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:10:32,304 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:10:32,306 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:10:37,307 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:11:00,310 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:11:00,312 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:11:05,296 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:11:05,303 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:11:05,307 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x76e6fdf11a90>
2025-06-17 07:11:05,307 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x76e70083d750> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:11:05,315 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x76e6fe934380>
2025-06-17 07:11:05,315 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:11:05,316 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:11:05,316 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:11:05,316 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:11:05,316 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:11:07,589 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:11:07 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1997'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1998'), (b'x-kong-proxy-latency', b'5'), (b'x-kong-request-id', b'f088f00f419e3f22eb71540b19333c66'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=8ucvdkgU.zohrOejzO8IXV2zYdsgvFMA98yFhlG39d8-1750144267-1.0.1.1-8M0O5GzUIB7jyn2tLg2OaKs8HTgGOXXALXe22rWQkBjOJCY0RW.CJP8JBDOZFfi0t2Ui59sQYDsCW8Qa6OBnWOLstcWJvIYoFQu_LxEIpgk; path=/; expires=Tue, 17-Jun-25 07:41:07 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=sGiGJGI4zFlhdA6ChXEo9VfRsBzMZbuKOhndCF7nOPI-1750144267593-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510bb1a4be0c058-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:11:07,589 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:11:07,590 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:11:07,591 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:11:07,591 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:11:07,591 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:11:13,158 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:11:13,162 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x76e6fdf11df0>
2025-06-17 07:11:13,162 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x76e70083d750> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:11:13,170 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x76e6fdf11010>
2025-06-17 07:11:13,170 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:11:13,171 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:11:13,171 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:11:13,171 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:11:13,171 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:11:14,670 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:11:14 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'643'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'645'), (b'x-kong-proxy-latency', b'6'), (b'x-kong-request-id', b'dc1ca55a05cd3a313b84317b85f9f00a'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510bb4b6950d1ce-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:11:14,670 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:11:14,671 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:11:23,355 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:11:23,356 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:11:23,358 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:11:23,695 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.330s]
2025-06-17 07:13:28,594 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:13:28,597 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:13:38,720 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:13:47,234 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: too many values to unpack (expected 3)
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 326, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 200, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: too many values to unpack (expected 3)
2025-06-17 07:14:40,902 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:14:40,904 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:14:46,822 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:14:47,708 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: too many values to unpack (expected 2)
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 326, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 197, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: too many values to unpack (expected 2)
2025-06-17 07:18:22,792 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:18:22,794 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:18:27,674 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:18:33,203 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:18:33,206 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:18:38,373 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:19:04,687 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:19:04,690 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:19:10,113 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:19:14,221 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:19:14,266 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7ac5a2b20920>
2025-06-17 07:19:14,266 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7ac5a54e56d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:19:14,273 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7ac5a2b218b0>
2025-06-17 07:19:14,273 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:19:14,273 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:19:14,273 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:19:14,274 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:19:14,274 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:19:14,955 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:19:14 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'394'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'395'), (b'x-kong-proxy-latency', b'8'), (b'x-kong-request-id', b'0aa36a2d9197216ecfc16b14b6f8dc4f'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Ugsr_poaoKIvSe0sTNT6vYdqAcofCF0oglhTjWtiXfk-1750144754-1.0.1.1-4BOhy55lg18_wBjiGUkwAM_FCWDqj2DSfNjSYWuNF3NAeyKRRv3SSEhqv_I90cNH9ZUkl59769jwjzL5m4wQuqPftd8qbvuRp2HQPM_19kk; path=/; expires=Tue, 17-Jun-25 07:49:14 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=cg8LTTz1AMxhbZhVb_RgeGQf3DVEMkkgnuPK9bK60U8-1750144754958-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510c70a48e2ea19-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:19:14,955 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:19:14,955 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:19:36,742 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:19:36,743 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:19:36,746 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:19:37,465 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.717s]
2025-06-17 07:20:52,003 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:20:52,006 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:21:21,992 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:21:22,453 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:21:22,455 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:21:27,454 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:21:27,461 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:21:27,465 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x79643a9218e0>
2025-06-17 07:21:27,465 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x79643d2e97d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:21:27,471 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x79643a921820>
2025-06-17 07:21:27,471 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:21:27,471 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:21:27,471 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:21:27,471 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:21:27,471 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:21:27,772 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 422, b'Unprocessable Entity', [(b'Date', b'Tue, 17 Jun 2025 07:21:27 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'8293'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'7'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'8'), (b'x-kong-proxy-latency', b'13'), (b'x-kong-request-id', b'64165c49b5956577aa0b124a8eacb52d'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=wSOxHDpZomz2DjmDouEfp2p.EXRno9rByhQAa_ZsJJQ-1750144887-1.0.1.1-17DT6ysAWEzEN410nKlM9sUGC8Litdl_LT27JYXA1RDftMxroZuXgq4jBuxmxO0ON7cQZQGVAJpFe_pBW7MPowcYqNNyXiPwF_rBdj_sDno; path=/; expires=Tue, 17-Jun-25 07:51:27 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=fVqoUvbp0qD7KzOaQfOEMttgpXYKGA4LNml0Y7aIKLU-1750144887777-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510ca4acba6d1d3-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:21:27,774 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 422 Unprocessable Entity"
2025-06-17 07:21:27,774 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:21:27,775 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 07:21:27,775 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:21:27,775 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:21:27,806 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 422 while fetching https://api.mistral.ai/v1/chat/completions: {"detail":[{"type":"value_error","loc":["body"],"msg":"Value error, Image content must be a URL (starting with 'https') or base64 encoded image (starting with 'data:image/<format>;base64,<image-base64>'). Received: /home/ljy/python_code/MCP/ollama-mcp-chat/context/132826.jpg","input":{"messages":[{"role":"user","content":[{"type":"text","text":"이미지의 설명을 해줘"},{"type":"image_url","image_url":"/home/ljy/python_code/MCP/ollama-mcp-chat/context/132826.jpg"}]}],"model":"llama4:17b-scout-16e-instruct-q8_0","temperature":0.1,"top_p":1,"tools":[{"type":"function","function":{"name":"get_local_file_list","description":"\n    Get a list of files and directories in a specified path.\n\n    Args:\n        path (str): local directory path to get file list\n\n    Returns:\n        str: A string containing the file list separated by newlines\n    ","parameters":{"properties":{"path":{"type":"string"}},"required":["path"],"type":"object"}}},{"type":"function","function":{"name":"write_text_to_file","description":"\n    Write specified text to a file.\n\n    Args:\n        file_name (str): The name of the file to write to\n        text (str): The text to write to the file\n\n    Returns:\n        str: A string containing the weather information for the specified location\n    ","parameters":{"properties":{"file_name":{"type":"string"},"text":{"type":"string"}},"required":["file_name","text"],"type":"object"}}},{"type":"function","function":{"name":"tavily-search","description":"A powerful web search tool that provides comprehensive, real-time results using Tavily's AI search engine. Returns relevant web content with customizable parameters for result count, content type, and domain filtering. Ideal for gathering current information, news, and detailed web content analysis.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"Search query"},"search_depth":{"type":"string","enum":["basic","advanced"],"description":"The depth of the search. It can be 'basic' or 'advanced'","default":"basic"},"topic":{"type":"string","enum":["general","news"],"description":"The category of the search. This will determine which of our agents will be used for the search","default":"general"},"days":{"type":"number","description":"The number of days back from the current date to include in the search results. This specifies the time frame of data to be retrieved. Please note that this feature is only available when using the 'news' search topic","default":3},"time_range":{"type":"string","description":"The time range back from the current date to include in the search results. This feature is available for both 'general' and 'news' search topics","enum":["day","week","month","year","d","w","m","y"]},"max_results":{"type":"number","description":"The maximum number of search results to return","default":10,"minimum":5,"maximum":20},"include_images":{"type":"boolean","description":"Include a list of query-related images in the response","default":false},"include_image_descriptions":{"type":"boolean","description":"Include a list of query-related images and their descriptions in the response","default":false},"include_raw_content":{"type":"boolean","description":"Include the cleaned and parsed HTML content of each search result","default":false},"include_domains":{"type":"array","items":{"type":"string"},"description":"A list of domains to specifically include in the search results, if the user asks to search on specific sites set this to the domain of the site","default":[]},"exclude_domains":{"type":"array","items":{"type":"string"},"description":"List of domains to specifically exclude, if the user asks to exclude a domain set this to the domain of the site","default":[]}},"required":["query"]}}},{"type":"function","function":{"name":"tavily-extract","description":"A powerful web content extraction tool that retrieves and processes raw content from specified URLs, ideal for data collection, content analysis, and research tasks.","parameters":{"type":"object","properties":{"urls":{"type":"array","items":{"type":"string"},"description":"List of URLs to extract content from"},"extract_depth":{"type":"string","enum":["basic","advanced"],"description":"Depth of extraction - 'basic' or 'advanced', if usrls are linkedin use 'advanced' or if explicitly told to use advanced","default":"basic"},"include_images":{"type":"boolean","description":"Include a list of images extracted from the urls in the response","default":false}},"required":["urls"]}}},{"type":"function","function":{"name":"tavily-crawl","description":"A powerful web crawler that initiates a structured web crawl starting from a specified base URL. The crawler expands from that point like a tree, following internal links across pages. You can control how deep and wide it goes, and guide it to focus on specific sections of the site.","parameters":{"type":"object","properties":{"url":{"type":"string","description":"The root URL to begin the crawl"},"max_depth":{"type":"integer","description":"Max depth of the crawl. Defines how far from the base URL the crawler can explore.","default":1,"minimum":1},"max_breadth":{"type":"integer","description":"Max number of links to follow per level of the tree (i.e., per page)","default":20,"minimum":1},"limit":{"type":"integer","description":"Total number of links the crawler will process before stopping","default":50,"minimum":1},"query":{"type":"string","description":"Natural language instructions for the crawler"},"select_paths":{"type":"array","items":{"type":"string"},"description":"Regex patterns to select only URLs with specific path patterns (e.g., /docs/.*, /api/v1.*)","default":[]},"select_domains":{"type":"array","items":{"type":"string"},"description":"Regex patterns to select crawling to specific domains or subdomains (e.g., ^docs\\.example\\.com$)","default":[]},"allow_external":{"type":"boolean","description":"Whether to allow following links that go to external domains","default":false},"categories":{"type":"array","items":{"type":"string","enum":["Careers","Blog","Documentation","About","Pricing","Community","Developers","Contact","Media"]},"description":"Filter URLs using predefined categories like documentation, blog, api, etc","default":[]},"extract_depth":{"type":"string","enum":["basic","advanced"],"description":"Advanced extraction retrieves more data, including tables and embedded content, with higher success but may increase latency","default":"basic"}},"required":["url"]}}},{"type":"function","function":{"name":"tavily-map","description":"A powerful web mapping tool that creates a structured map of website URLs, allowing you to discover and analyze site structure, content organization, and navigation paths. Perfect for site audits, content discovery, and understanding website architecture.","parameters":{"type":"object","properties":{"url":{"type":"string","description":"The root URL to begin the mapping"},"max_depth":{"type":"integer","description":"Max depth of the mapping. Defines how far from the base URL the crawler can explore","default":1,"minimum":1},"max_breadth":{"type":"integer","description":"Max number of links to follow per level of the tree (i.e., per page)","default":20,"minimum":1},"limit":{"type":"integer","description":"Total number of links the crawler will process before stopping","default":50,"minimum":1},"query":{"type":"string","description":"Natural language instructions for the crawler"},"select_paths":{"type":"array","items":{"type":"string"},"description":"Regex patterns to select only URLs with specific path patterns (e.g., /docs/.*, /api/v1.*)","default":[]},"select_domains":{"type":"array","items":{"type":"string"},"description":"Regex patterns to select crawling to specific domains or subdomains (e.g., ^docs\\.example\\.com$)","default":[]},"allow_external":{"type":"boolean","description":"Whether to allow following links that go to external domains","default":false},"categories":{"type":"array","items":{"type":"string","enum":["Careers","Blog","Documentation","About","Pricing","Community","Developers","Contact","Media"]},"description":"Filter URLs using predefined categories like documentation, blog, api, etc","default":[]}},"required":["url"]}}}],"stream":true},"ctx":{"error":{}}}]}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 326, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 197, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 422 while fetching https://api.mistral.ai/v1/chat/completions: {"detail":[{"type":"value_error","loc":["body"],"msg":"Value error, Image content must be a URL (starting with 'https') or base64 encoded image (starting with 'data:image/<format>;base64,<image-base64>'). Received: /home/ljy/python_code/MCP/ollama-mcp-chat/context/132826.jpg","input":{"messages":[{"role":"user","content":[{"type":"text","text":"이미지의 설명을 해줘"},{"type":"image_url","image_url":"/home/ljy/python_code/MCP/ollama-mcp-chat/context/132826.jpg"}]}],"model":"llama4:17b-scout-16e-instruct-q8_0","temperature":0.1,"top_p":1,"tools":[{"type":"function","function":{"name":"get_local_file_list","description":"\n    Get a list of files and directories in a specified path.\n\n    Args:\n        path (str): local directory path to get file list\n\n    Returns:\n        str: A string containing the file list separated by newlines\n    ","parameters":{"properties":{"path":{"type":"string"}},"required":["path"],"type":"object"}}},{"type":"function","function":{"name":"write_text_to_file","description":"\n    Write specified text to a file.\n\n    Args:\n        file_name (str): The name of the file to write to\n        text (str): The text to write to the file\n\n    Returns:\n        str: A string containing the weather information for the specified location\n    ","parameters":{"properties":{"file_name":{"type":"string"},"text":{"type":"string"}},"required":["file_name","text"],"type":"object"}}},{"type":"function","function":{"name":"tavily-search","description":"A powerful web search tool that provides comprehensive, real-time results using Tavily's AI search engine. Returns relevant web content with customizable parameters for result count, content type, and domain filtering. Ideal for gathering current information, news, and detailed web content analysis.","parameters":{"type":"object","properties":{"query":{"type":"string","description":"Search query"},"search_depth":{"type":"string","enum":["basic","advanced"],"description":"The depth of the search. It can be 'basic' or 'advanced'","default":"basic"},"topic":{"type":"string","enum":["general","news"],"description":"The category of the search. This will determine which of our agents will be used for the search","default":"general"},"days":{"type":"number","description":"The number of days back from the current date to include in the search results. This specifies the time frame of data to be retrieved. Please note that this feature is only available when using the 'news' search topic","default":3},"time_range":{"type":"string","description":"The time range back from the current date to include in the search results. This feature is available for both 'general' and 'news' search topics","enum":["day","week","month","year","d","w","m","y"]},"max_results":{"type":"number","description":"The maximum number of search results to return","default":10,"minimum":5,"maximum":20},"include_images":{"type":"boolean","description":"Include a list of query-related images in the response","default":false},"include_image_descriptions":{"type":"boolean","description":"Include a list of query-related images and their descriptions in the response","default":false},"include_raw_content":{"type":"boolean","description":"Include the cleaned and parsed HTML content of each search result","default":false},"include_domains":{"type":"array","items":{"type":"string"},"description":"A list of domains to specifically include in the search results, if the user asks to search on specific sites set this to the domain of the site","default":[]},"exclude_domains":{"type":"array","items":{"type":"string"},"description":"List of domains to specifically exclude, if the user asks to exclude a domain set this to the domain of the site","default":[]}},"required":["query"]}}},{"type":"function","function":{"name":"tavily-extract","description":"A powerful web content extraction tool that retrieves and processes raw content from specified URLs, ideal for data collection, content analysis, and research tasks.","parameters":{"type":"object","properties":{"urls":{"type":"array","items":{"type":"string"},"description":"List of URLs to extract content from"},"extract_depth":{"type":"string","enum":["basic","advanced"],"description":"Depth of extraction - 'basic' or 'advanced', if usrls are linkedin use 'advanced' or if explicitly told to use advanced","default":"basic"},"include_images":{"type":"boolean","description":"Include a list of images extracted from the urls in the response","default":false}},"required":["urls"]}}},{"type":"function","function":{"name":"tavily-crawl","description":"A powerful web crawler that initiates a structured web crawl starting from a specified base URL. The crawler expands from that point like a tree, following internal links across pages. You can control how deep and wide it goes, and guide it to focus on specific sections of the site.","parameters":{"type":"object","properties":{"url":{"type":"string","description":"The root URL to begin the crawl"},"max_depth":{"type":"integer","description":"Max depth of the crawl. Defines how far from the base URL the crawler can explore.","default":1,"minimum":1},"max_breadth":{"type":"integer","description":"Max number of links to follow per level of the tree (i.e., per page)","default":20,"minimum":1},"limit":{"type":"integer","description":"Total number of links the crawler will process before stopping","default":50,"minimum":1},"query":{"type":"string","description":"Natural language instructions for the crawler"},"select_paths":{"type":"array","items":{"type":"string"},"description":"Regex patterns to select only URLs with specific path patterns (e.g., /docs/.*, /api/v1.*)","default":[]},"select_domains":{"type":"array","items":{"type":"string"},"description":"Regex patterns to select crawling to specific domains or subdomains (e.g., ^docs\\.example\\.com$)","default":[]},"allow_external":{"type":"boolean","description":"Whether to allow following links that go to external domains","default":false},"categories":{"type":"array","items":{"type":"string","enum":["Careers","Blog","Documentation","About","Pricing","Community","Developers","Contact","Media"]},"description":"Filter URLs using predefined categories like documentation, blog, api, etc","default":[]},"extract_depth":{"type":"string","enum":["basic","advanced"],"description":"Advanced extraction retrieves more data, including tables and embedded content, with higher success but may increase latency","default":"basic"}},"required":["url"]}}},{"type":"function","function":{"name":"tavily-map","description":"A powerful web mapping tool that creates a structured map of website URLs, allowing you to discover and analyze site structure, content organization, and navigation paths. Perfect for site audits, content discovery, and understanding website architecture.","parameters":{"type":"object","properties":{"url":{"type":"string","description":"The root URL to begin the mapping"},"max_depth":{"type":"integer","description":"Max depth of the mapping. Defines how far from the base URL the crawler can explore","default":1,"minimum":1},"max_breadth":{"type":"integer","description":"Max number of links to follow per level of the tree (i.e., per page)","default":20,"minimum":1},"limit":{"type":"integer","description":"Total number of links the crawler will process before stopping","default":50,"minimum":1},"query":{"type":"string","description":"Natural language instructions for the crawler"},"select_paths":{"type":"array","items":{"type":"string"},"description":"Regex patterns to select only URLs with specific path patterns (e.g., /docs/.*, /api/v1.*)","default":[]},"select_domains":{"type":"array","items":{"type":"string"},"description":"Regex patterns to select crawling to specific domains or subdomains (e.g., ^docs\\.example\\.com$)","default":[]},"allow_external":{"type":"boolean","description":"Whether to allow following links that go to external domains","default":false},"categories":{"type":"array","items":{"type":"string","enum":["Careers","Blog","Documentation","About","Pricing","Community","Developers","Contact","Media"]},"description":"Filter URLs using predefined categories like documentation, blog, api, etc","default":[]}},"required":["url"]}}}],"stream":true},"ctx":{"error":{}}}]}
2025-06-17 07:24:47,747 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:24:47,749 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:27:54,725 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:27:55,182 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:27:55,185 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:29:01,253 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:29:01,711 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:29:01,713 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:29:06,562 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:29:09,946 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:29:09,995 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fdbbc110fe0>
2025-06-17 07:29:09,995 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fdbbd7d97d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:29:10,011 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fdbbb7116d0>
2025-06-17 07:29:10,012 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:29:10,012 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:29:10,013 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:29:10,013 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:29:10,013 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:29:10,613 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Tue, 17 Jun 2025 07:29:10 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'130'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'128'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'129'), (b'x-kong-proxy-latency', b'201'), (b'x-kong-request-id', b'a16a454805b57547374a9b5ac1d4b246'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=aQzUWsxWlZVYoHBSsCZQyZbWeeWW8PYz1PjqtW2uin4-1750145350-1.0.1.1-rxYQNAUGoEkpB.ZuNHO.Pr7Kx27g2HcYa8pz7HwGyqOiwDKJKuF53RLvCrIg.dtFt8Qo1JTKQZ6tb8IB1fl79GFHs4Fh1b.YbNN9_ePNfh4; path=/; expires=Tue, 17-Jun-25 07:59:10 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=EuejjHigwr3un4ha1FjkvDzWwBdooJWHfn7OdqiTmZg-1750145350619-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510d595aa30d1ee-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:29:10,614 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-06-17 07:29:10,614 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:29:10,614 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-17 07:29:10,614 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:29:10,614 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:29:10,619 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:17b-scout-16e-instruct-q8_0","type":"invalid_model","param":null,"code":"1500"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 326, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:17b-scout-16e-instruct-q8_0","type":"invalid_model","param":null,"code":"1500"}
2025-06-17 07:31:26,370 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:31:26,372 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:32:11,375 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:32:11,840 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:32:11,843 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:32:17,768 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:32:45,971 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:32:45,973 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:32:51,119 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:32:51,583 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:32:51,586 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:32:56,716 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:33:05,099 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:33:05,103 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72a9b0118aa0>
2025-06-17 07:33:05,103 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72a9b21e18d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:33:05,114 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72a9b0be6d50>
2025-06-17 07:33:05,114 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:33:05,114 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:33:05,114 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:33:05,115 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:33:05,115 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:33:07,313 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:33:07 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1044'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1045'), (b'x-kong-proxy-latency', b'303'), (b'x-kong-request-id', b'c7f35b1d8efc1675e582ed442165960d'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=.uV6eT7QzeMJwFgzuzGhjuMarhFKYamm0M9wiU1tWwY-1750145587-1.0.1.1-qUs5G8yPyY2Mn4XbPBYmfPGSKP95ZBmo7FG3NAPTy3MCi3Bj.WTp2QJ.OaIXu.OtwuqS7yl1ZL9vEWnw334KaXrYY0F5yxB5RY5p3Q8VeD8; path=/; expires=Tue, 17-Jun-25 08:03:07 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=kTNc4UPOri_CREgCSx_iQgVtNhe9OVv0IaXkZP6qUwA-1750145587318-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510db530ec0a7d7-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:33:07,315 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:33:07,315 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:33:08,602 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:33:08,603 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:33:08,604 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:33:09,012 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.407s]
2025-06-17 07:37:12,805 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:37:12,855 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72a9b14a0800>
2025-06-17 07:37:12,855 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72a9b21e18d0> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:37:12,862 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72a9b0b30410>
2025-06-17 07:37:12,863 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:37:12,863 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:37:12,863 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:37:12,869 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:37:12,869 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:37:15,386 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:37:15 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1597'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1599'), (b'x-kong-proxy-latency', b'615'), (b'x-kong-request-id', b'17d9a83167caf574dde2b952cb80165d'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510e15f6d19c102-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:37:15,386 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:37:15,387 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:37:16,986 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:37:16,986 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:37:16,986 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:37:17,984 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.995s]
2025-06-17 07:38:15,954 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:38:15,956 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:41:35,954 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:41:36,413 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:41:36,415 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:41:38,614 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:41:40,067 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:41:40,070 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72fc4dff02c0>
2025-06-17 07:41:40,071 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72fc4fe59750> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:41:40,079 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72fc4dfc9ee0>
2025-06-17 07:41:40,079 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:41:40,079 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:41:40,079 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:41:40,080 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:41:40,080 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:41:40,512 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:41:40 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'144'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'144'), (b'x-kong-proxy-latency', b'10'), (b'x-kong-request-id', b'd2d65b0d7245b627cb64501de7a06a78'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=nAXimKfR55Lw9LX9PkXFoAQwBTxM6lqdl4Lunc7kRz4-1750146100-1.0.1.1-HkuXU_Ghi47e.EaQ5Ux0hdJR5gExMgkGIDUiIrIf2K90R6T0hyPOW7BJ4IXzCK_eDwpswl1kD5q1K1qfqGYdsC7uAkZNpH5m24uO_6jw3EA; path=/; expires=Tue, 17-Jun-25 08:11:40 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=8y4PRSIPMuL8trBJgJ71O9DAEm7Bjn20TY8oDbFuByY-1750146100509-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510e7e58dac30af-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:41:40,514 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:41:40,514 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:41:54,145 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:41:54,145 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:41:54,146 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:41:54,431 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.283s]
2025-06-17 07:48:15,527 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:48:15,529 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:48:17,730 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:48:28,745 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:48:28,747 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:48:30,947 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:48:35,081 - api.api_server - ERROR - 오류 발생: '\n  "image_description"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 280, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n  "image_description"'
2025-06-17 07:48:47,889 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:48:47,891 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:48:50,109 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:50:17,018 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:50:17,021 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:50:19,223 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:50:21,590 - api.api_server - ERROR - 오류 발생: '\n  "image_description"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 280, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n  "image_description"'
2025-06-17 07:50:39,664 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:50:39,667 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 07:50:41,873 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 07:50:42,838 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 07:50:42,889 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x79cc4c9e12e0>
2025-06-17 07:50:42,889 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x79cc4dfcd750> server_hostname='api.mistral.ai' timeout=120
2025-06-17 07:50:42,896 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x79cc4c9e12b0>
2025-06-17 07:50:42,896 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 07:50:42,896 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 07:50:42,896 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 07:50:42,896 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 07:50:42,896 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 07:50:43,339 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 07:50:43 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'173'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'173'), (b'x-kong-proxy-latency', b'8'), (b'x-kong-request-id', b'554f0b042d279b25395b060699d29129'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=5kwR61fOxI_vw72CW.eeUy0fm3KiJGkw8ABx2QYYyv4-1750146643-1.0.1.1-aXxuJ_QOKrzxMu8B_vppNOyCAO9LoF5F_x4G4c18cm1HTThpinRESD6YgVrjmWR0Ku1K5Dkap5huaj5po9pT1ICR5VjtHWWiPbKXodfhh6A; path=/; expires=Tue, 17-Jun-25 08:20:43 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=LfgTR9irjaKj6gBD_PSeSyYy1j.0Db3ZVr_XmiiTit8-1750146643332-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9510f5261a3da422-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 07:50:43,340 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 07:50:43,340 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 07:50:57,171 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 07:50:57,171 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 07:50:57,172 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 07:50:57,250 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.075s]
2025-06-17 08:41:05,703 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:41:05,705 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:41:07,947 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.045s]
2025-06-17 08:41:09,678 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 08:41:09,723 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x76c0e39e0590>
2025-06-17 08:41:09,723 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x76c0e4fcd750> server_hostname='api.mistral.ai' timeout=120
2025-06-17 08:41:09,732 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x76c0e39e1220>
2025-06-17 08:41:09,732 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 08:41:09,732 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 08:41:09,732 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 08:41:09,732 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 08:41:09,732 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 08:41:10,168 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 08:41:10 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'151'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'151'), (b'x-kong-proxy-latency', b'7'), (b'x-kong-request-id', b'83d9c411e2a9c71395c87b1c4f4ac11b'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=nvJcFuojK4JhZgzwdOKsM5.4WS0zPdLLsb3tTAfYDTM-1750149670-1.0.1.1-Dtj7DPGN46wYeXnmOW.c.WfJYRJzQtyXAO7EFQQS14d.TxD0DpNdqqjsALXyHLYyOFuicTCFZHvEld6loVWMFD4Uxb9FTPeDGsyrGMWbJoQ; path=/; expires=Tue, 17-Jun-25 09:11:10 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=GNbl7AZZ6P1_mtyhmYXZJTKETgxOjISj0I3nBKU6ICY-1750149670176-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95113f0bec683170-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 08:41:10,170 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 08:41:10,176 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 08:41:31,527 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 08:41:31,528 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 08:41:31,532 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 08:41:31,735 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.195s]
2025-06-17 08:42:04,609 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:42:04,611 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:42:06,816 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 08:43:01,445 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:43:01,448 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:43:03,646 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 08:43:07,947 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:43:07,949 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:43:10,154 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 08:43:11,295 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 08:43:11,302 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7422fb6bdaf0>
2025-06-17 08:43:11,302 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7422fc35d750> server_hostname='api.mistral.ai' timeout=120
2025-06-17 08:43:11,315 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7422fad99100>
2025-06-17 08:43:11,315 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 08:43:11,316 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 08:43:11,316 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 08:43:11,316 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 08:43:11,317 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 08:43:35,869 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 08:43:35 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'24280'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'24281'), (b'x-kong-proxy-latency', b'7'), (b'x-kong-request-id', b'fd9c5d4fcb2f1d5987ab7827cc225112'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=jGOLubS64qoUR0JlUSI9p52KXpGPHVi6Qj2BlGPaPFE-1750149815-1.0.1.1-3kbfxC4oHL_zzFPwJpDCHhDn7505OTGIhZXJoDVjbuBPwTutpC2brr8YmA_PGdggc.OM7WZ59jxPFX9Hc7CGTDWwTlnVCesjJuOnBfaapvU; path=/; expires=Tue, 17-Jun-25 09:13:35 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=IGrPxsNRiIyHpcPwJETC_o12f_loLCXF3ATV3HZhJSs-1750149815875-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95114203c8a5a7cf-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 08:43:35,870 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 08:43:35,871 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 08:43:59,837 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 08:43:59,837 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 08:43:59,842 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 08:44:00,435 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.585s]
2025-06-17 08:50:06,058 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:50:06,060 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:50:08,265 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 08:50:09,127 - api.api_server - ERROR - 오류 발생: '\n  "image_description"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 263, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n  "image_description"'
2025-06-17 08:50:41,240 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:50:41,242 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:50:43,442 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 08:50:44,790 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 08:50:44,837 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7644f3f182f0>
2025-06-17 08:50:44,837 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7644f54f9750> server_hostname='api.mistral.ai' timeout=120
2025-06-17 08:50:44,844 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7644f3f18f50>
2025-06-17 08:50:44,844 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 08:50:44,844 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 08:50:44,844 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 08:50:44,844 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 08:50:44,844 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 08:50:45,370 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 08:50:45 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'195'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'195'), (b'x-kong-proxy-latency', b'6'), (b'x-kong-request-id', b'4bdc88338f9f830cc8e2b243c6786e49'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=CVA.HxdU8BrieXly6X_.mP1RTVm4AG6oE0Ec4gxPqkI-1750150245-1.0.1.1-7ZmT4ysT5SDmblJnwpeyRmfx441VDdIUDYEmPQBRs6ahm3FcPjy_r2wPnv__LgPlcFGQG1AK.twDSDpwc0mEW6w1H25uweMexDMvyHOdPmw; path=/; expires=Tue, 17-Jun-25 09:20:45 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=sZrtu5O8KM60Tf5lH_ZeGZb1sk144zhX3KwvzeG1_IE-1750150245368-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95114d164d1ad1de-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 08:50:45,371 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 08:50:45,372 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 08:50:56,735 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 08:50:56,735 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 08:50:56,738 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 08:50:57,012 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.269s]
2025-06-17 08:51:32,416 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:51:32,418 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:51:34,624 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 08:51:37,472 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 08:51:37,476 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a5bc299c650>
2025-06-17 08:51:37,476 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7a5bc3f89750> server_hostname='api.mistral.ai' timeout=120
2025-06-17 08:51:37,484 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a5bc299d280>
2025-06-17 08:51:37,484 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 08:51:37,484 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 08:51:37,484 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 08:51:37,484 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 08:51:37,484 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 08:51:37,919 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 08:51:37 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'155'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'156'), (b'x-kong-proxy-latency', b'7'), (b'x-kong-request-id', b'5da66723a39cfa473d417b317fd86425'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ilYlM74V5CCthpAU5iOLf3Bwyq8mv96qtk.HGRuhDQ8-1750150297-1.0.1.1-kFCWgdDK0JbLeFoDFXKiL_3rjoJJJZWULrkwZV2EVCiapuKxCbz9lu3v5qWN1REpoE2lgLyEpgxl_k3RkBRDIWA_wpri5kQMWwZTSduESB8; path=/; expires=Tue, 17-Jun-25 09:21:37 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=bVsFzVFkpNLqj3NrAOW3S8I.FZQRAIwoPnk9QzY.1AA-1750150297918-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95114e5f4b2730aa-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 08:51:37,920 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 08:51:37,920 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 08:52:00,060 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 08:52:00,061 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 08:52:00,065 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 08:52:00,135 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.061s]
2025-06-17 08:52:58,232 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:52:58,234 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 08:53:00,435 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 08:53:05,212 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-17 08:53:05,215 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x77def01d43b0>
2025-06-17 08:53:05,215 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x77def17b1750> server_hostname='api.mistral.ai' timeout=120
2025-06-17 08:53:05,224 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x77def01d5280>
2025-06-17 08:53:05,224 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-17 08:53:05,224 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-17 08:53:05,224 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-17 08:53:05,224 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-17 08:53:05,224 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-17 08:53:05,622 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 17 Jun 2025 08:53:05 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'109'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'110'), (b'x-kong-proxy-latency', b'7'), (b'x-kong-request-id', b'd1d82353b4e4a604f8ba5e616139cf01'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=QY07tNUTA6UB2gzmmQLxwKEWSq1LyuyyGcxPlVP6IHE-1750150385-1.0.1.1-XUJ5bqC_tdmYlFQP6ss6HKDPUgXvgqTp9hdba1xw_RKCGChwRYuSK1ZrBJfVeZr.xclzPcg5K5ksnxI.HlLb2K5lV0EsVYeRpAqB1szlFaE; path=/; expires=Tue, 17-Jun-25 09:23:05 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=DUJJJuZcwnihxvIXFrunYPhPKFY2kIZVDQ5Y9q_9VE8-1750150385619-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95115083ae0530a9-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-17 08:53:05,623 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-17 08:53:05,623 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-17 08:53:20,409 - httpcore.http11 - DEBUG - response_closed.started
2025-06-17 08:53:20,409 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-17 08:53:20,414 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-17 08:53:20,920 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.499s]
2025-06-17 09:10:31,630 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 09:10:31,632 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 09:10:33,879 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.044s]
2025-06-17 09:10:43,402 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 09:10:43,404 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 09:10:45,605 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-17 09:11:14,061 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 09:11:14,063 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-17 09:11:16,268 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 00:00:38,704 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 00:00:41,790 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x79e19cdf1190>
2025-06-18 00:00:41,795 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x79e19e3e17d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 00:00:41,802 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x79e19cdf10d0>
2025-06-18 00:00:41,802 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 00:00:41,802 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 00:00:41,802 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 00:00:41,803 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 00:00:41,803 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 00:00:43,600 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 00:00:43 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'670'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'672'), (b'x-kong-proxy-latency', b'288'), (b'x-kong-request-id', b'aa9f1b7194c751510ccb6315e5752817'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=4T7FtyWnQDik_026b.K1gnN.Uxy5AFSQsvTzc_QIwrA-1750204843-1.0.1.1-KTDDwp1PQSVMEUttOHB2nWs4HFJZKhqPRmaJIrr4FRkjSNGifPQoJp0XQNG2HiVwz.kP82DzchyGEmqBF46jqrGNdsjHKlHnIZIHliAnv0M; path=/; expires=Wed, 18-Jun-25 00:30:43 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=MfF_5Gw1uZ9GHQsyyHhexQeA1kx_Oek3nwUANhn4sLs-1750204843603-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951682054fa2c10f-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 00:00:43,602 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 00:00:43,602 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 00:00:48,487 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 00:00:48,487 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 00:00:48,491 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 00:00:48,496 - api.api_server - ERROR - 오류 발생: name 'explicit_command' is not defined
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 314, in chat_endpoint
    title=explicit_command[:10] + "...",
          ^^^^^^^^^^^^^^^^
NameError: name 'explicit_command' is not defined
2025-06-18 00:00:55,609 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 00:00:55,611 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 00:00:57,866 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.044s]
2025-06-18 00:01:06,687 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 00:01:06,691 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x70991cbe52e0>
2025-06-18 00:01:06,692 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x70991e1d57d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 00:01:06,699 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x70991cbe5220>
2025-06-18 00:01:06,700 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 00:01:06,700 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 00:01:06,700 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 00:01:06,700 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 00:01:06,700 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 00:01:07,846 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 00:01:07 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'707'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'707'), (b'x-kong-proxy-latency', b'171'), (b'x-kong-request-id', b'4aca484c9508da187294d8714c55494a'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=VXQ60AxyKOeJMVw87HkYOxIHJxbS8dk9CPsolMUq4bI-1750204867-1.0.1.1-78b3NJZxJlI0zq3vwLz1wzenIIz.97LdYAR11g86nJvxsqBVjkOfn5vKjovRac6PAe8LGkL8hDPeVchmZ7uWeBzrWBQLfzwQFmcBNXiHKz8; path=/; expires=Wed, 18-Jun-25 00:31:07 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=PZHGJUjH4O8grlZVcQz.f.p81q9BVxLwE6ISGf0lOCc-1750204867852-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951682a0ed7cd1cf-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 00:01:07,847 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 00:01:07,847 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 00:01:13,805 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 00:01:13,805 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 00:01:13,808 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 00:01:13,815 - api.api_server - ERROR - 오류 발생: name 'explicit_command' is not defined
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 314, in chat_endpoint
    title=explicit_command[:10] + "...",
          ^^^^^^^^^^^^^^^^
NameError: name 'explicit_command' is not defined
2025-06-18 00:01:31,866 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 00:01:31,868 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 00:01:34,072 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 00:01:42,380 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 00:01:42,385 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104fddd3d0>
2025-06-18 00:01:42,385 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x741051cb97d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 00:01:42,395 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104fdde990>
2025-06-18 00:01:42,395 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 00:01:42,395 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 00:01:42,396 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 00:01:42,396 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 00:01:42,396 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 00:01:43,179 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 00:01:43 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'445'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'446'), (b'x-kong-proxy-latency', b'72'), (b'x-kong-request-id', b'ee0c4d60796a726da51cdb2f886e4031'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=F5oCiO4mTy9Dl8uI1jMbH06lJNZukKg8z8pBda3WKqU-1750204903-1.0.1.1-wti6xitFwOgKmV3VyUVfktP1Ugtgeb1nS6QjGkTJ8x6m87iJUdTDfj4WJOpVSRGexgNZ70WMZSY200s6rl6HaaPt9j5B6V30V8j20y68eto; path=/; expires=Wed, 18-Jun-25 00:31:43 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=QU7CqlrVGABWA0HBcLX8.upt53KQsgqgM6a_Vhkfhs0-1750204903182-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9516838008e3311b-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 00:01:43,181 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 00:01:43,181 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 00:01:43,835 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 00:01:43,836 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 00:01:43,838 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 00:01:44,473 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.627s]
2025-06-18 04:08:55,477 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:08:55,525 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x741050719c40>
2025-06-18 04:08:55,525 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x741051cb97d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:08:55,537 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104fdde0f0>
2025-06-18 04:08:55,538 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:08:55,538 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:08:55,538 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:08:55,545 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:08:55,545 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:08:57,235 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:08:57 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1214'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1215'), (b'x-kong-proxy-latency', b'209'), (b'x-kong-request-id', b'5e6fc622a15d0f258e1439782688d04d'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=JdiKLTcvC8.vrIaSjCuf225FIs9xRRPHz5ZmUqZFioA-1750219737-1.0.1.1-EEQpGSIhGqpRdGCPiBk5Uj6ZofRyzyjuCmxpoq84nxCogdO3V10UA1QxbixlEuDZC9.BMFg4OA3QX9QNjWlQAMyFWx_jpVzrF50QASU6174; path=/; expires=Wed, 18-Jun-25 04:38:57 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9517eda32fdca7db-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:08:57,236 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:08:57,237 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:08:59,220 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:08:59,220 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:08:59,221 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:08:59,232 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.010s]
2025-06-18 04:08:59,235 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.002s]
2025-06-18 04:08:59,418 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_update/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.182s]
2025-06-18 04:09:53,662 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:09:53,665 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104f32f6e0>
2025-06-18 04:09:53,665 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x741051cb97d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:09:53,672 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104fd96630>
2025-06-18 04:09:53,672 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:09:53,672 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:09:53,672 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:09:53,677 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:09:53,677 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:09:56,201 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:09:56 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'2067'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2067'), (b'x-kong-proxy-latency', b'186'), (b'x-kong-request-id', b'eb69507ce8c126094a422d530888781d'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9517ef0e789ad1cf-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:09:56,202 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:09:56,202 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:09:58,728 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:09:58,728 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:09:58,731 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:09:58,740 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.004s]
2025-06-18 04:09:58,747 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.007s]
2025-06-18 04:09:59,015 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_update/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.267s]
2025-06-18 04:11:20,478 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:11:20,482 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104f32fce0>
2025-06-18 04:11:20,482 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x741051cb97d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:11:20,493 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104f32ffe0>
2025-06-18 04:11:20,493 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:11:20,494 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:11:20,494 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:11:20,499 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:11:20,499 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:11:23,343 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:11:23 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'2331'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2332'), (b'x-kong-proxy-latency', b'234'), (b'x-kong-request-id', b'923fdf6214d83a0404517d16244d4d28'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9517f12d1b633273-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:11:23,343 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:11:23,343 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:11:25,536 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:11:25,536 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:11:25,538 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:11:25,556 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.015s]
2025-06-18 04:11:25,557 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.001s]
2025-06-18 04:11:25,831 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_update/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.273s]
2025-06-18 04:12:02,196 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:12:02,200 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104f309130>
2025-06-18 04:12:02,200 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x741051cb97d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:12:02,208 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104f30bdd0>
2025-06-18 04:12:02,208 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:12:02,208 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:12:02,208 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:12:02,214 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:12:02,214 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:12:05,146 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:12:05 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1877'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1878'), (b'x-kong-proxy-latency', b'768'), (b'x-kong-request-id', b'759a7f7e9b72fd4423189f770dc9724e'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9517f231ce223116-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:12:05,146 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:12:05,146 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:12:08,108 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:12:08,108 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:12:08,109 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:12:08,120 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.009s]
2025-06-18 04:12:08,121 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.001s]
2025-06-18 04:12:08,344 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_update/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.224s]
2025-06-18 04:14:20,405 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:14:20,455 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104f348140>
2025-06-18 04:14:20,455 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x741051cb97d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:14:20,466 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104f34aea0>
2025-06-18 04:14:20,467 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:14:20,467 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:14:20,467 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:14:20,510 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:14:20,510 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:14:23,399 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:14:23 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1947'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1948'), (b'x-kong-proxy-latency', b'680'), (b'x-kong-request-id', b'4561345e9fa26c3777f5de1535189439'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9517f591ec98d1d6-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:14:23,399 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:14:23,400 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:14:25,402 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:14:25,402 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:14:25,402 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:14:25,413 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.009s]
2025-06-18 04:14:25,414 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.001s]
2025-06-18 04:14:25,468 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_update/mwxXgJcBoTRDRCFccXKn [status:200 duration:0.053s]
2025-06-18 04:15:13,297 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:15:13,301 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104f391610>
2025-06-18 04:15:13,301 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x741051cb97d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:15:13,310 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74104f352390>
2025-06-18 04:15:13,311 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:15:13,311 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:15:13,311 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:15:13,384 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:15:13,384 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:15:16,639 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:15:16 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'2291'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2293'), (b'x-kong-proxy-latency', b'694'), (b'x-kong-request-id', b'7c421b3e2ab188206214f423aa266b56'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9517f6dc2c0fea21-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:15:16,640 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:15:16,640 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:15:20,454 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:15:20,454 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:15:20,454 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:15:20,723 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.267s]
2025-06-18 04:15:27,434 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:15:27,436 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:15:29,637 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:15:41,194 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:15:41,651 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:15:41,653 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:15:43,853 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:15:45,979 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:15:45,983 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e624496a600>
2025-06-18 04:15:45,983 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7e6245f597d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:15:45,990 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e6244994aa0>
2025-06-18 04:15:45,991 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:15:45,991 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:15:45,991 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:15:46,023 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:15:46,023 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:15:47,491 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:15:47 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'654'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'655'), (b'x-kong-proxy-latency', b'550'), (b'x-kong-request-id', b'84fb7a1effd1327973bc4e0cf0c00342'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=RGel9sb9cp4XoAmc39MPHmbQOM6BEt2K1WaMNy6ZnDQ-1750220147-1.0.1.1-C3FMqMpAORLlMTt0bGfBk4_YNoer3pTQf3waGc7Qj98vHMuliuoPm7U5_9DP.Wqc5JRQ2c_dwVdcv3hg14LRuckNwpuB0vNEZ5ALhyhJZkc; path=/; expires=Wed, 18-Jun-25 04:45:47 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=5W.17k_aQ0biEfVNUoBRcZj6jnFObxerwCE_.5lXHKY-1750220147485-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9517f7a86d86c107-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:15:47,491 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:15:47,491 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:15:47,981 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:15:47,981 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:15:47,984 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:15:48,516 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.524s]
2025-06-18 04:16:28,276 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:16:28,280 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e6244994f20>
2025-06-18 04:16:28,280 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7e6245f597d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:16:28,289 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e6244968110>
2025-06-18 04:16:28,289 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:16:28,289 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:16:28,289 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:16:28,315 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:16:28,315 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:16:30,124 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:16:30 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'744'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'746'), (b'x-kong-proxy-latency', b'786'), (b'x-kong-request-id', b'295b8c42dc0e832d1ed1b12f8031a865'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9517f8b0cda5ea23-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:16:30,124 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:16:30,125 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:16:32,153 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:16:32,154 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:16:32,157 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:16:32,167 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/nQxAgZcBoTRDRCFcDXIa [status:200 duration:0.004s]
2025-06-18 04:16:32,171 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/chat_history/_doc/nQxAgZcBoTRDRCFcDXIa [status:200 duration:0.003s]
2025-06-18 04:16:32,439 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_update/nQxAgZcBoTRDRCFcDXIa [status:200 duration:0.268s]
2025-06-18 04:29:47,693 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:29:47,736 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e6243e99100>
2025-06-18 04:29:47,737 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7e6245f597d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:29:47,748 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e6246f8e720>
2025-06-18 04:29:47,748 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:29:47,749 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:29:47,749 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:29:47,772 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:29:47,773 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:29:49,908 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:29:49 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1059'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1059'), (b'x-kong-proxy-latency', b'802'), (b'x-kong-request-id', b'1ddbe77e7a98fb610288e747aa736909'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95180c356a693122-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:29:49,909 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:29:49,909 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:29:51,616 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:29:51,616 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:29:51,617 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:29:51,678 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.060s]
2025-06-18 04:30:27,740 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:30:27,743 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e6243e99610>
2025-06-18 04:30:27,744 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7e6245f597d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:30:27,754 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e6243eb4e30>
2025-06-18 04:30:27,754 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:30:27,754 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:30:27,754 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:30:27,780 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:30:27,780 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:30:29,144 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:30:29 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'577'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'578'), (b'x-kong-proxy-latency', b'521'), (b'x-kong-request-id', b'3ab62f6a95a6740e26526d24b3699e32'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95180d2f6ceb3284-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:30:29,144 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:30:29,144 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:30:29,739 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:30:29,739 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:30:29,742 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:30:29,800 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.052s]
2025-06-18 04:30:57,848 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:30:57,852 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e6243eb7710>
2025-06-18 04:30:57,852 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7e6245f597d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:30:57,863 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e6243eb6060>
2025-06-18 04:30:57,863 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:30:57,864 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:30:57,864 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:30:57,896 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:30:57,897 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:30:59,458 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:30:59 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'778'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'779'), (b'x-kong-proxy-latency', b'514'), (b'x-kong-request-id', b'56978116577642039094b0711d296033'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=eBR9ZYUYYO8spfTPWAkPccJcUwWOM_VfAtdE7NOUwB4-1750221059-1.0.1.1-NjUqKVe0YJnybYNv9i88xhVl4qjDV.G7Nx93AofMxREaTR36BEDfOHps6UGc6UzuBuQ.rjRpUPq7nGtPY0lDFkH5l7nNCU_Nr2SJGl_vPhU; path=/; expires=Wed, 18-Jun-25 05:00:59 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95180deb9eef3277-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:30:59,459 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:30:59,459 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:31:00,073 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:31:00,073 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:31:00,076 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:31:00,407 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.325s]
2025-06-18 04:31:06,209 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:31:06,213 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:31:08,415 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:31:10,023 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:31:10,025 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:31:12,238 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:31:13,030 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:31:13,033 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:31:15,236 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:31:16,431 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:31:16,433 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:31:18,635 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:31:21,513 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:31:21,516 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7b50e051ca40>
2025-06-18 04:31:21,516 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7b50e1afd750> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:31:21,524 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7b50e051d250>
2025-06-18 04:31:21,524 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:31:21,525 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:31:21,525 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:31:21,525 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:31:21,525 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:31:24,154 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:31:24 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'2341'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2342'), (b'x-kong-proxy-latency', b'6'), (b'x-kong-request-id', b'd877190f32fe8b01d7f83e69575fc167'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=.FmUC5C86.43pI7ddv7oxZ_tnpt4.l3XJc9E5ZvUBus-1750221084-1.0.1.1-SZgS2ZGxDASlDwuk8Ea18WVYDKw.Z7mktiN3DZn7oI3lfNSaPPbsTdbJ2wMIOvNUQ2pZLYT5kIGzsW7UKX7n8VY8ujBtERkFQCdXgDvC8XY; path=/; expires=Wed, 18-Jun-25 05:01:24 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=zDE2L7_k5uXCDfaMUzeVUUvUJjDhovcuPB1EsB4nRow-1750221084143-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95180e7f78beaa48-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:31:24,155 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:31:24,155 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:31:40,724 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:31:40,724 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:31:40,729 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:31:40,790 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.055s]
2025-06-18 04:35:43,421 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:35:43,423 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:35:45,632 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:35:48,831 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:35:48,907 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x77fd4d4e0a70>
2025-06-18 04:35:48,907 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x77fd4ecb5750> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:35:48,916 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x77fd4d4e1310>
2025-06-18 04:35:48,916 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:35:48,917 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:35:48,917 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:35:48,917 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:35:48,917 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:35:49,274 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:35:49 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'89'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'90'), (b'x-kong-proxy-latency', b'5'), (b'x-kong-request-id', b'0fc0b96f7b1bb32e882e4732a384b065'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Xq3b070SB5H4O7iCTHN9ZpZInkNsI029Q.alVhRO1CI-1750221349-1.0.1.1-agwYntJ9LGesNgAPHA3I7Bun1gLvuHxRng7aE5.Y.KT3bBjZcomqjzxsNR2WQ8lL91zLHHdw3SedITpNt_OznMrJG1cYUQpV_7nkaOytjV4; path=/; expires=Wed, 18-Jun-25 05:05:49 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=saMQPjELXV8i6Q7OXSRm3JICCiOf0lFcNz6_IQ.oqzs-1750221349262-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95181506ae6bc10c-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:35:49,274 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:35:49,274 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:35:56,284 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:35:56,284 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:35:56,284 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:35:56,347 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.061s]
2025-06-18 04:38:35,807 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:38:36,264 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:38:36,267 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:38:38,465 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:39:00,211 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:39:00,213 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:39:02,425 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:39:13,322 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:39:13,324 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:39:15,530 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:39:29,450 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:39:29,454 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7581535db4a0>
2025-06-18 04:39:29,454 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x758154b5d7d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:39:29,464 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75815356e600>
2025-06-18 04:39:29,464 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:39:29,464 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:39:29,465 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:39:29,465 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:39:29,465 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:39:30,727 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:39:30 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'996'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'996'), (b'x-kong-proxy-latency', b'5'), (b'x-kong-request-id', b'eaecac02054ac466113d1e4a274cdb57'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=zxLd1iRtYxw3s21hVXbk4slHszlOSSQRzDrz1o7uWmQ-1750221570-1.0.1.1-LBxFHiFaf3ZrZ7itgJ70n8.MbM8tjZPDbDH2w5PP93YjEE0Uz1jfB4f5Beyi1q3erRTT.VygglyygvH52IH7jekOY0d0yPUsU7Ff6f75EO8; path=/; expires=Wed, 18-Jun-25 05:09:30 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=4xa2phs_HHXcVvU1TaTZSXml8rmDVEtWFx73G8g8Tgs-1750221570719-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95181a692d883097-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:39:30,727 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:39:30,727 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:39:30,728 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:39:30,728 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:39:30,729 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:39:30,736 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:39:30,739 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x758153bfda00>
2025-06-18 04:39:30,739 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x758154b5d7d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:39:30,746 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x758153596f90>
2025-06-18 04:39:30,747 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:39:30,747 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:39:30,747 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:39:30,747 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:39:30,747 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:39:32,026 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:39:32 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1008'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1009'), (b'x-kong-proxy-latency', b'7'), (b'x-kong-request-id', b'11140663f29b6c6377421a42a3e01347'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95181a712b11ea26-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:39:32,026 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:39:32,027 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:39:36,787 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:39:36,787 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:39:36,789 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:39:36,860 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.065s]
2025-06-18 04:41:28,700 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:41:28,702 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:41:30,913 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:41:35,508 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:41:35,511 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:41:37,711 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:41:39,926 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:41:39,974 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x761b0b3ed3d0>
2025-06-18 04:41:39,974 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x761b0c9e17d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:41:39,982 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x761b0b3ed310>
2025-06-18 04:41:39,982 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:41:39,982 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:41:39,982 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:41:39,982 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:41:39,982 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:41:58,488 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:41:58 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'18200'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'18201'), (b'x-kong-proxy-latency', b'4'), (b'x-kong-request-id', b'8526f14db5dd9d745ad69e055da5c870'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ElfefWjjYNod8PWAMuqlcKTfQH9KfS.45Cqz9Ooubao-1750221718-1.0.1.1-SZ0YKooaPotTmgeY7KN9g9tv_MlGey0bvyqymm0rW9YHLrgfo9K0OuDWFqB.YPr57SajfAzRfvcXB0eXGOcgfmsDAV3IlaASDiDu1RVaRtw; path=/; expires=Wed, 18-Jun-25 05:11:58 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=mkFiHQvfKuCsmpF4Uc2juAJtN0ugpUrWqPN3Ubqsy8M-1750221718483-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95181d98ec0abcc4-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:41:58,488 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:41:58,489 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:41:58,489 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:41:58,489 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:41:58,490 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:41:58,496 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:41:58,500 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x761b0b3edd90>
2025-06-18 04:41:58,500 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x761b0c9e17d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:41:58,508 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x761b0b3eee10>
2025-06-18 04:41:58,508 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:41:58,508 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:41:58,508 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:41:58,508 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:41:58,508 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:41:59,140 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:41:59 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'360'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'361'), (b'x-kong-proxy-latency', b'7'), (b'x-kong-request-id', b'0608b6641675037d90214e7f73c42755'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95181e0cae2cea2b-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:41:59,140 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:41:59,140 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:42:01,887 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:42:01,887 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:42:01,887 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:42:02,495 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.606s]
2025-06-18 04:42:22,133 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:42:22,135 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:42:24,338 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:42:27,157 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:42:27,161 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72e347f05190>
2025-06-18 04:42:27,164 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72e349fc17d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:42:27,174 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72e347f05070>
2025-06-18 04:42:27,174 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:42:27,174 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:42:27,174 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:42:27,174 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:42:27,174 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:42:39,778 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:42:39 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'12336'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'12337'), (b'x-kong-proxy-latency', b'5'), (b'x-kong-request-id', b'feab9f3abe99553b6963a52d42fd7a05'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=fqjXhWtKH6ZJMar9v6wru4_fy8m8K21tPKLnx7aqjMY-1750221759-1.0.1.1-Xb0a1whL1i0vF6gGjnA04xAmjzquaAUbtJTDci30XonfO0pJl675VBX33Df0unyYa_y36AhwUgb6aGuNQYMnsFpW.5pmQXAcEKg5qvymBK8; path=/; expires=Wed, 18-Jun-25 05:12:39 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=ReieF8OPES0QCb3PALIz7bgmzvaeQAI6xHEEcm7qmos-1750221759773-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95181ebfdadcaa5f-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:42:39,778 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:42:39,778 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:42:43,568 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:42:43,568 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:42:43,569 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:42:43,872 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.302s]
2025-06-18 04:44:28,129 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:44:28,136 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72e347f04e90>
2025-06-18 04:44:28,136 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72e349fc17d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:44:28,147 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72e348a59f10>
2025-06-18 04:44:28,148 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:44:28,148 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:44:28,148 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:44:28,149 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:44:28,149 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:44:29,503 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:44:29 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'1089'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1090'), (b'x-kong-proxy-latency', b'5'), (b'x-kong-request-id', b'36a64e7b01cffd3c73665810e2358c5c'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951821b3ebe5ea12-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:44:29,504 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:44:29,504 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:44:29,508 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:44:29,508 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:44:29,511 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:44:29,529 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:44:29,533 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72e348a28860>
2025-06-18 04:44:29,533 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72e349fc17d0> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:44:29,542 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72e3488d4b30>
2025-06-18 04:44:29,542 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:44:29,542 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:44:29,542 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:44:29,542 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:44:29,542 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:44:45,563 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:44:45 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'15749'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'15750'), (b'x-kong-proxy-latency', b'4'), (b'x-kong-request-id', b'1d8eaf069ee42168804fb05ef09cf159'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951821bca81dea2b-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:44:45,564 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:44:45,564 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:44:46,423 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:44:46,424 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:44:46,433 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:44:46,482 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.046s]
2025-06-18 04:45:14,612 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:45:14,615 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:45:16,825 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:45:27,327 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:45:27,329 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:45:29,538 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:45:38,156 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:45:38,158 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-18 04:45:40,363 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-06-18 04:45:44,559 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-06-18 04:45:44,563 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x710b8b9f4410>
2025-06-18 04:45:44,563 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x710b8d841750> server_hostname='api.mistral.ai' timeout=120
2025-06-18 04:45:44,572 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x710b8b9f5100>
2025-06-18 04:45:44,572 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-18 04:45:44,572 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-18 04:45:44,572 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-18 04:45:44,572 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-18 04:45:44,572 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-18 04:45:45,381 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 18 Jun 2025 04:45:45 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'x-envoy-upstream-service-time', b'527'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'527'), (b'x-kong-proxy-latency', b'6'), (b'x-kong-request-id', b'f5e84f62cc1afd541dbc7e1176aaa341'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=uP.ifL4i4TeoOhRGQjDkz9yQOOIjfsIgj1pzYLufFk0-1750221945-1.0.1.1-MjAr.OVJ3YKE_xKcHAxHSZ60YPGD1Q0VGzV.kQJt2w09FNOCCse9dviJKmHN9LjbUjwwINMhSUa7ZCmN_ywlfJQ9v4Q4NtJEetWh99Il1mw; path=/; expires=Wed, 18-Jun-25 05:15:45 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=7jmAVff96c5fvDsPxNtjcA5o6stkMmL5so3oXd5KM88-1750221945380-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'951823919f5bc076-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-18 04:45:45,382 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-18 04:45:45,382 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-18 04:46:08,577 - httpcore.http11 - DEBUG - response_closed.started
2025-06-18 04:46:08,578 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-18 04:46:08,579 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-06-18 04:46:08,658 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.076s]
2025-06-25 04:17:25,582 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-25 04:17:26,036 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-25 04:17:26,038 - asyncio - DEBUG - Using selector: EpollSelector
2025-06-25 04:17:31,862 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.045s]
2025-08-04 05:54:03,137 - asyncio - DEBUG - Using selector: EpollSelector
2025-08-04 05:54:03,599 - asyncio - DEBUG - Using selector: EpollSelector
2025-08-04 05:54:03,610 - asyncio - DEBUG - Using selector: EpollSelector
2025-08-04 06:16:20,896 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.045s]
2025-08-04 06:16:36,420 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 06:16:36,503 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75e516932d20>
2025-08-04 06:16:36,503 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x75e517f4ced0> server_hostname='api.mistral.ai' timeout=120
2025-08-04 06:16:36,518 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75e516932cc0>
2025-08-04 06:16:36,518 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 06:16:36,518 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 06:16:36,518 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 06:16:36,518 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 06:16:36,518 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 06:16:36,900 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Mon, 04 Aug 2025 06:16:36 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'130'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873b9-8c53-76ed-9f6d-38d3a099b168'), (b'x-kong-request-id', b'019873b9-8c53-76ed-9f6d-38d3a099b168'), (b'x-envoy-upstream-service-time', b'43'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'44'), (b'x-kong-proxy-latency', b'7'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=IuZkzHziu4LpL2HzbgSgh6cVNMJ1vNdHK3W39xHl2gk-1754288196-1.0.1.1-uGs_GUFnNwaw8ZdkMKgD.0MOGEtXp3nmcGixv.VedkTRjqS1WEQgYSB6WgWLN2vJ5dNiCzyiUorgMTOUFakrrfuvHUMc9F_J1mZzWdMWDjk; path=/; expires=Mon, 04-Aug-25 06:46:36 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=J8Ehnm_z4nXbEa80lrjX0TdPTCWC8l5Gc0J9vkFK1uo-1754288196892-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969bed4c3ecfaa6c-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 06:16:36,901 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-04 06:16:36,901 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 06:16:36,901 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 06:16:36,901 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 06:16:36,901 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 06:16:36,921 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:17b-scout-16e-instruct-q8_0","type":"invalid_model","param":null,"code":"1500"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 288, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:17b-scout-16e-instruct-q8_0","type":"invalid_model","param":null,"code":"1500"}
2025-08-04 06:24:35,394 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:25:08,145 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 06:25:08,203 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x736c89555ac0>
2025-08-04 06:25:08,203 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x736c8aba4ed0> server_hostname='api.mistral.ai' timeout=120
2025-08-04 06:25:08,216 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x736c89555a00>
2025-08-04 06:25:08,217 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 06:25:08,217 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 06:25:08,218 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 06:25:08,218 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 06:25:08,218 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 06:25:08,575 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Mon, 04 Aug 2025 06:25:08 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'130'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873c1-5b12-7a88-97e3-bd8d6ba88da6'), (b'x-kong-request-id', b'019873c1-5b12-7a88-97e3-bd8d6ba88da6'), (b'x-envoy-upstream-service-time', b'44'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'45'), (b'x-kong-proxy-latency', b'7'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=B09GwsvZT1SS72xwlwqFFDNHeY4a1mtbNhvaPH8Czuk-1754288708-1.0.1.1-.E6PgP7Ef76szc5fHNK98dBDEExk8lsmIMkwezV4A2aI2uVixwcQ_Hd_q8owBX16yM0vpfSLYk6w3kk5MG2B0X9Wdpbbwomm98ACaAbJqT0; path=/; expires=Mon, 04-Aug-25 06:55:08 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=fqHtWhgexI7ASocB0mDYpAX3YWOwMd0n6V5HMvPEvoI-1754288708565-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969bf9ca5b72e9fd-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 06:25:08,577 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-04 06:25:08,577 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 06:25:08,577 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 06:25:08,577 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 06:25:08,578 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 06:25:08,595 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:17b-scout-16e-instruct-q8_0","type":"invalid_model","param":null,"code":"1500"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 288, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:17b-scout-16e-instruct-q8_0","type":"invalid_model","param":null,"code":"1500"}
2025-08-04 06:25:24,518 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:25:26,067 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 06:25:26,074 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7bee71dc1af0>
2025-08-04 06:25:26,074 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7bee73470ed0> server_hostname='api.mistral.ai' timeout=120
2025-08-04 06:25:26,090 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7bee71d9e9f0>
2025-08-04 06:25:26,090 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 06:25:26,091 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 06:25:26,091 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 06:25:26,091 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 06:25:26,091 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 06:25:27,184 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Mon, 04 Aug 2025 06:25:27 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'108'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873c1-a383-7cbd-8df6-cb8a6dee118f'), (b'x-kong-request-id', b'019873c1-a383-7cbd-8df6-cb8a6dee118f'), (b'x-envoy-upstream-service-time', b'45'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'46'), (b'x-kong-proxy-latency', b'5'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=pfW.7Oab6dcVAl1CgieLQ1Vp5MZ5dG5XJyzf5kwInQo-1754288727-1.0.1.1-KmB7xiEporlxBYgDl1M8QFCt_ZsPkx_T4VAnx9P8IARAPezOCeCp1H7ssYXLGHUi3JVNA_0IXvDdJuQZkQQgfz74qHcNmjE8ljf009u8pgo; path=/; expires=Mon, 04-Aug-25 06:55:27 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=9CDkjy9rRXaam8bbSIe_FvWsYEJs3LuSrK5XSDwVHio-1754288727171-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969bfa3a080eea92-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 06:25:27,186 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-04 06:25:27,186 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 06:25:27,186 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 06:25:27,187 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 06:25:27,187 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 06:25:27,205 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:scout","type":"invalid_model","param":null,"code":"1500"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 288, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:scout","type":"invalid_model","param":null,"code":"1500"}
2025-08-04 06:26:48,725 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:26:53,911 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:26:56,287 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 06:26:56,292 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a270cf1d8b0>
2025-08-04 06:26:56,295 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7a270e554ed0> server_hostname='api.mistral.ai' timeout=120
2025-08-04 06:26:56,309 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a270cf1d880>
2025-08-04 06:26:56,309 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 06:26:56,310 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 06:26:56,310 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 06:26:56,310 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 06:26:56,310 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 06:26:56,674 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Mon, 04 Aug 2025 06:26:56 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'119'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873c3-0157-7d67-8e02-f3a86c3f2b16'), (b'x-kong-request-id', b'019873c3-0157-7d67-8e02-f3a86c3f2b16'), (b'x-envoy-upstream-service-time', b'61'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'62'), (b'x-kong-proxy-latency', b'6'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=X6X12ZBwgSJ.rvUvg9U5yN9eJVqM0jEJ2yxl31QU8wE-1754288816-1.0.1.1-1HiokBv5fFPgnXD6eaQ.vr4j_Lh8HRy7FolurAFUIICIXo7qXu.SGm7CGdy5pfx4RJuochH.ZyblP29kX4WArP3q.kbPtzZDv1yXwI_G8_o; path=/; expires=Mon, 04-Aug-25 06:56:56 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=8jvFkkknZmzFXR8QbXnKEda3UZJRHr9w6afQlYo3KX4-1754288816665-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969bfc6def523109-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 06:26:56,676 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-04 06:26:56,676 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 06:26:56,676 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 06:26:56,676 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 06:26:56,677 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 06:26:56,692 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: mistral-small3.1:latest","type":"invalid_model","param":null,"code":"1500"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 288, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: mistral-small3.1:latest","type":"invalid_model","param":null,"code":"1500"}
2025-08-04 06:27:05,112 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:27:10,810 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:27:13,474 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:27:16,135 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:28:43,484 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 06:28:43,491 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7d5fe10e2480>
2025-08-04 06:28:43,491 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7d5fe27a4ed0> server_hostname='api.mistral.ai' timeout=120
2025-08-04 06:28:43,500 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7d5fe19ffaa0>
2025-08-04 06:28:43,500 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 06:28:43,501 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 06:28:43,501 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 06:28:43,541 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 06:28:43,541 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 06:28:44,606 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Mon, 04 Aug 2025 06:28:44 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'130'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873c4-a42d-7f86-8726-15da0483d756'), (b'x-kong-request-id', b'019873c4-a42d-7f86-8726-15da0483d756'), (b'x-envoy-upstream-service-time', b'53'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'54'), (b'x-kong-proxy-latency', b'698'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=yvgW3UbjmJF_NqSNgzFC20uMb2akQPr_isBpH6Oqr9E-1754288924-1.0.1.1-JJSScVdl2yB13WkbwmiweGqlaAgePsbX11gWxzTkSPasKrzdNzcM0t0KwLtxiieO1.gDg0b8pNwAyEf1bwDSyw0ltuVVBSHtwBPtVhhzT6o; path=/; expires=Mon, 04-Aug-25 06:58:44 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=ok.jPsS3fBgJE31PFfnuwTLzsFnCeGwpF4yB9ffopnk-1754288924595-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969bff0bdf1eea1d-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 06:28:44,608 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-04 06:28:44,609 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 06:28:44,609 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 06:28:44,610 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 06:28:44,610 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 06:28:44,627 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:17b-scout-16e-instruct-q8_0","type":"invalid_model","param":null,"code":"1500"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 288, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Invalid model: llama4:17b-scout-16e-instruct-q8_0","type":"invalid_model","param":null,"code":"1500"}
2025-08-04 06:29:39,958 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:32:12,198 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.002s]
2025-08-04 06:32:22,911 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:33:59,043 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:33:59,868 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 06:33:59,959 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a96b0bc5910>
2025-08-04 06:33:59,959 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7a96b2274ed0> server_hostname='api.mistral.ai' timeout=120
2025-08-04 06:33:59,967 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a96b0bc5850>
2025-08-04 06:33:59,967 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 06:33:59,968 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 06:33:59,968 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 06:34:00,005 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 06:34:00,006 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 06:34:00,973 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Mon, 04 Aug 2025 06:34:00 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'137'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873c9-786a-721a-be0b-cf9c0db136d8'), (b'x-kong-request-id', b'019873c9-786a-721a-be0b-cf9c0db136d8'), (b'x-envoy-upstream-service-time', b'91'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'92'), (b'x-kong-proxy-latency', b'549'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=BKxw3roo.7aLAuG4naU3DlSTEP3XyGwP7HCo38AbwQI-1754289240-1.0.1.1-PAoP.LyXTdrJLbzTXFNq1FHM6KYbu6TayW.WL5gBOWbW_xpCwifpSA0YZ3G_iM51Sp0fzYzULD1CbyuSq8GSc105z5zZgABYRXtwbYYBkVw; path=/; expires=Mon, 04-Aug-25 07:04:00 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=kfD58r2WscdlLHdCX7at7lqBozH8TSlzhWgxDZOcmoQ-1754289240965-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c06c5cb69d1e5-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 06:34:00,975 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-04 06:34:00,975 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 06:34:00,975 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 06:34:00,976 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 06:34:00,976 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 06:34:00,996 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Image input is not enabled for this model","type":"invalid_request_invalid_args","param":null,"code":"3051"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 288, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 400 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Image input is not enabled for this model","type":"invalid_request_invalid_args","param":null,"code":"3051"}
2025-08-04 06:35:03,201 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 06:35:06,178 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 06:35:06,193 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7aca1333be30>
2025-08-04 06:35:06,193 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7aca143bced0> server_hostname='api.mistral.ai' timeout=120
2025-08-04 06:35:06,212 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7aca12d99550>
2025-08-04 06:35:06,213 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 06:35:06,213 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 06:35:06,214 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 06:35:06,521 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 06:35:06,521 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 06:35:11,697 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 04 Aug 2025 06:35:11 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873ca-7f60-7bc0-9913-f38c2c968bb4'), (b'x-kong-request-id', b'019873ca-7f60-7bc0-9913-f38c2c968bb4'), (b'x-envoy-upstream-service-time', b'2892'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2893'), (b'x-kong-proxy-latency', b'1107'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=eC6farQahFpY6oxEUAQaG9DuQRmCMTx8B8ufGPomsAs-1754289311-1.0.1.1-up10SwqKCPCJ9RHB8Cg7gvi8YFc_cxEKw.cYrOqLBX3GJgwoisxPcNTS0KlNwo3UJbPk0xyp2HRbwTqa.CAJl1RgJXEClvRwzYHFGNRmV4E; path=/; expires=Mon, 04-Aug-25 07:05:11 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=7KfDHVM8NFyQMHArD.DZIDTyqGMXS5cpTdfAWC.jMBQ-1754289311688-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c0863df81ea1d-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 06:35:11,699 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-04 06:35:11,699 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 06:35:12,956 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 06:35:12,956 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 06:35:12,957 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-08-04 06:35:13,343 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.385s]
2025-08-04 06:38:40,858 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 06:38:40,864 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7aca12e22480>
2025-08-04 06:38:40,864 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7aca143bced0> server_hostname='api.mistral.ai' timeout=120
2025-08-04 06:38:40,874 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7aca12cfb620>
2025-08-04 06:38:40,874 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 06:38:40,874 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 06:38:40,874 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 06:38:40,971 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 06:38:40,971 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 06:38:44,135 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 04 Aug 2025 06:38:44 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873cd-c1eb-7ff5-af31-2ff40ea71807'), (b'x-kong-request-id', b'019873cd-c1eb-7ff5-af31-2ff40ea71807'), (b'x-envoy-upstream-service-time', b'2131'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'2133'), (b'x-kong-proxy-latency', b'735'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c0da17d1fea20-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 06:38:44,136 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-04 06:38:44,137 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 06:38:46,096 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 06:38:46,096 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 06:38:46,097 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-08-04 06:38:46,463 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.365s]
2025-08-04 07:02:36,969 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.044s]
2025-08-04 07:02:42,168 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 07:02:44,922 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 07:02:44,967 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7664a1b058e0>
2025-08-04 07:02:44,967 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7664a3c68e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 07:02:44,974 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7664a27942f0>
2025-08-04 07:02:44,974 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 07:02:44,974 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 07:02:44,974 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 07:02:44,975 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 07:02:44,975 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 07:02:47,009 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 04 Aug 2025 07:02:47 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873e3-ca90-772a-9025-b1aaf377be86'), (b'x-kong-request-id', b'019873e3-ca90-772a-9025-b1aaf377be86'), (b'x-envoy-upstream-service-time', b'1720'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'1721'), (b'x-kong-proxy-latency', b'6'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=T98rTUsZHrZaO7Znotaje4TAynqOwqAa1IEQaiowPrE-1754290966-1.0.1.1-lmc7fcF9d62xLlNx0P5455zynQDDm6PDXegLevQgmkwYkcJx4nsL3q2z86q32sScCcsNQw6nOg_OOA4eJlHw9bBRZVhSLfnSQuBn2fqVjts; path=/; expires=Mon, 04-Aug-25 07:32:46 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=0lzh22s9.cGVsaB_HeY7Ovads63K.fBlxKOmIjy83OY-1754290967003-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c30e318c1e9f7-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 07:02:47,010 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-04 07:02:47,011 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 07:02:54,632 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 07:02:54,633 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 07:02:54,636 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-08-04 07:02:54,925 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.281s]
2025-08-04 07:10:02,463 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 07:10:06,980 - api.api_server - ERROR - 오류 발생: '\n  "image_description"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 296, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n  "image_description"'
2025-08-04 07:12:33,035 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 07:12:37,170 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 07:12:37,222 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72502806e210>
2025-08-04 07:12:37,222 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x725029558e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 07:12:37,236 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x725027ef9d60>
2025-08-04 07:12:37,237 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 07:12:37,237 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 07:12:37,237 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 07:12:37,238 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 07:12:37,238 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 07:12:38,127 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 04 Aug 2025 07:12:38 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873ec-d412-7e98-95e2-f7ccdaa5ed55'), (b'x-kong-request-id', b'019873ec-d412-7e98-95e2-f7ccdaa5ed55'), (b'x-envoy-upstream-service-time', b'596'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'597'), (b'x-kong-proxy-latency', b'6'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=0AellbqOqvV5MSfOa5hMJs6etbK7vAdrEsZCfUgCaes-1754291558-1.0.1.1-NeMTuQrQTDSYLyfFttll0OL5zqPwuvKMmJ2ynPFtstN0dVxYiZc0xaKsnhezx.fGefS6kXa3T92nljH1Uqcmws_bvb.5.c2Dwrv4a.E0mkI; path=/; expires=Mon, 04-Aug-25 07:42:38 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=PrkePohfJxSnTynS7_mIccB8HiSxET4DugLJYUJgqUU-1754291558125-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c3f58b98eaa61-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 07:12:38,127 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-04 07:12:38,128 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 07:12:45,658 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 07:12:45,659 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 07:12:45,662 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-08-04 07:12:45,741 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.071s]
2025-08-04 07:14:34,896 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 07:14:34,899 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x725027fa8fe0>
2025-08-04 07:14:34,899 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x725029558e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 07:14:34,908 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72502a5e72c0>
2025-08-04 07:14:34,908 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 07:14:34,908 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 07:14:34,908 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 07:14:34,908 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 07:14:34,908 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 07:14:35,497 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 04 Aug 2025 07:14:35 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'019873ee-9fbc-716f-b9d1-455cd3651e7b'), (b'x-kong-request-id', b'019873ee-9fbc-716f-b9d1-455cd3651e7b'), (b'x-envoy-upstream-service-time', b'284'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'285'), (b'x-kong-proxy-latency', b'17'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c42383911d1d6-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 07:14:35,498 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-04 07:14:35,498 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 07:14:41,387 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 07:14:41,387 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 07:14:41,390 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-08-04 07:14:41,447 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.051s]
2025-08-04 07:56:58,013 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 07:56:58,060 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x725027f19190>
2025-08-04 07:56:58,060 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x725029558e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 07:56:58,072 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x725027f1bef0>
2025-08-04 07:56:58,072 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 07:56:58,073 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 07:56:58,073 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 07:56:58,073 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 07:56:58,073 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 07:56:59,231 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 04 Aug 2025 07:56:59 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'01987415-6de7-7d58-b5b8-3636f91bcbf2'), (b'x-kong-request-id', b'01987415-6de7-7d58-b5b8-3636f91bcbf2'), (b'x-envoy-upstream-service-time', b'873'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'874'), (b'x-kong-proxy-latency', b'8'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=KRcXXbQSBOg0oN9HX2vOxpyZZ63UfWR68fjEV9hypJg-1754294219-1.0.1.1-7MyPf4tk9ihtI5.2gWqQI4BUMKFpiOsLQ2runOzmv1r4aD5stuUjn29wdT_te4fPyFXVd3Zh6Ekm.xxtOYoYF4PdFd129cRT5dGr0rmbr40; path=/; expires=Mon, 04-Aug-25 08:26:59 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c804f09e5d1ce-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 07:56:59,232 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-04 07:56:59,232 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 07:57:00,159 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 07:57:00,159 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 07:57:00,160 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-08-04 07:57:00,479 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.317s]
2025-08-04 08:05:48,738 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 08:05:55,324 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 08:05:55,372 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72cae93d9b50>
2025-08-04 08:05:55,372 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x72caeaa80e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 08:05:55,384 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x72cae93d9a60>
2025-08-04 08:05:55,384 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 08:05:55,385 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 08:05:55,385 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 08:05:55,385 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 08:05:55,385 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 08:05:55,743 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Mon, 04 Aug 2025 08:05:55 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'144'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'0198741d-a0d9-779f-9d7d-986f008ced19'), (b'x-kong-request-id', b'0198741d-a0d9-779f-9d7d-986f008ced19'), (b'x-envoy-upstream-service-time', b'60'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'61'), (b'x-kong-proxy-latency', b'7'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=yzsXnbVcAvcnGgDP4gRLbEOs2qLanAHFy3VQIgVedKg-1754294755-1.0.1.1-0jh4Nl3KOO3B6KYBxEyVqa8HUyDNQB3gHCIpo5i8pMOpOwbSjfxw8vPOalUYfEddTzdEETijqRBF2xulpMRm9bdi7LaN4IZ25BfnUwX6Xuo; path=/; expires=Mon, 04-Aug-25 08:35:55 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=ZRewQ0Zq_06XbMY8Zzcp7B3eyFopmV.C7HOz78rLjVc-1754294755749-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c8d6d3d16e9fd-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 08:05:55,745 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-08-04 08:05:55,745 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 08:05:55,746 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 08:05:55,746 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 08:05:55,746 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 08:05:55,766 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 288, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
2025-08-04 08:06:20,016 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 08:06:22,699 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 08:06:24,341 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 08:06:24,345 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75c2f98f64e0>
2025-08-04 08:06:24,345 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x75c2faf58e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 08:06:24,350 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x75c2f987fa40>
2025-08-04 08:06:24,350 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 08:06:24,350 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 08:06:24,350 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 08:06:24,351 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 08:06:24,351 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 08:06:24,856 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 04 Aug 2025 08:06:24 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'0198741e-1200-79be-9af6-0955c6042d51'), (b'x-kong-request-id', b'0198741e-1200-79be-9af6-0955c6042d51'), (b'x-envoy-upstream-service-time', b'205'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'206'), (b'x-kong-proxy-latency', b'7'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=yZ0VPiTgYlWQu.kR5HpccvEJWVCBex1A0c.NS8C_EaE-1754294784-1.0.1.1-Gw.gy3pw6IMHwjYOdJadNoR96SjJUeRsJXGXy8vGri5uMy0oM5Gkt0qfjuA82unbjNhAgpNS0n.RnPC2V6PyuuAEQxF5KLyydXL6b9Z0Fzc; path=/; expires=Mon, 04-Aug-25 08:36:24 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=vmSWPuMxc.imSO3.XINqsr7GE39MUkU.NLNM3GIGmuc-1754294784862-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c8e224ba2e9fb-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 08:06:24,858 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-04 08:06:24,858 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 08:06:31,621 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 08:06:31,621 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 08:06:31,625 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-08-04 08:06:31,690 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.056s]
2025-08-04 08:10:50,947 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 08:10:54,310 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 08:10:54,316 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x76f1ce7c2d20>
2025-08-04 08:10:54,317 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x76f1d0660e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 08:10:54,330 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x76f1ce7e9a30>
2025-08-04 08:10:54,330 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 08:10:54,331 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 08:10:54,331 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 08:10:54,331 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 08:10:54,331 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 08:10:54,679 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Mon, 04 Aug 2025 08:10:54 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'144'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'01987422-308f-770c-a84d-c44d2282d555'), (b'x-kong-request-id', b'01987422-308f-770c-a84d-c44d2282d555'), (b'x-envoy-upstream-service-time', b'54'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'55'), (b'x-kong-proxy-latency', b'6'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=aKRmAEkcQw4E_PmEXRrvThMa9AMiC4xDGnpZScllXbY-1754295054-1.0.1.1-HCkjKvi1dAvX5BXagL051gdQLqhRJuP_EVDBpCW7EJyFmUJvk.D0Gpz1VIwhwBQ2qbHAduWjsPgrGOC4PgXQ2o8gxc0SoZmT_D7JOpC_K8E; path=/; expires=Mon, 04-Aug-25 08:40:54 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=jSGWMiNFJPJJDDT3j8BBLDTEPOn3iERr6_WisilNhfo-1754295054681-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c94b99d99351a-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 08:10:54,681 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-08-04 08:10:54,681 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 08:10:54,681 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 08:10:54,681 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 08:10:54,682 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 08:10:54,699 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 292, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
2025-08-04 08:11:03,154 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 08:11:06,302 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 08:11:06,350 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74a145cf6150>
2025-08-04 08:11:06,350 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x74a147360e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 08:11:06,356 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x74a145cf6e10>
2025-08-04 08:11:06,356 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 08:11:06,356 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 08:11:06,356 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 08:11:06,356 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 08:11:06,356 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 08:11:06,828 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Mon, 04 Aug 2025 08:11:06 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'144'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'01987422-5f8e-7fa6-a2c2-e13f0bdd65cd'), (b'x-kong-request-id', b'01987422-5f8e-7fa6-a2c2-e13f0bdd65cd'), (b'x-envoy-upstream-service-time', b'173'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'174'), (b'x-kong-proxy-latency', b'5'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=iGnal7RMKKSE.Xn4hFQl3l_9wGx2QbOs4Z8y8uFq_Mo-1754295066-1.0.1.1-KBLBy.Zi_7V1WAZCrk40QYXrS49pjO6myroLMlrdWaeten9n83S98Dks6QJ7fRaRm5Rs_v3BZDntgFAMpMLN36ZpXZk3CQ_MC1sp_sAV.xc; path=/; expires=Mon, 04-Aug-25 08:41:06 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=FDz0Ehr3nC4N5ClHmvpsle0ZRbss4R91FQNMMDn6ZJs-1754295066830-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c9504cf99d1de-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 08:11:06,830 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-08-04 08:11:06,830 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 08:11:06,831 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 08:11:06,831 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 08:11:06,831 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 08:11:06,852 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 292, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
2025-08-04 08:11:15,974 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 08:11:19,200 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 08:11:19,204 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x723e345d9790>
2025-08-04 08:11:19,204 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x723e35c24e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 08:11:19,210 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x723e345d9760>
2025-08-04 08:11:19,210 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 08:11:19,210 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 08:11:19,210 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 08:11:19,210 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 08:11:19,210 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 08:11:19,567 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Mon, 04 Aug 2025 08:11:19 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'144'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'01987422-91cf-73c7-b104-4c968a8ba5e2'), (b'x-kong-request-id', b'01987422-91cf-73c7-b104-4c968a8ba5e2'), (b'x-envoy-upstream-service-time', b'48'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'49'), (b'x-kong-proxy-latency', b'5'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=4GfZpHqfhiHMoECMN6f.ApXslzk3thG2tKsrNRGY9Qg-1754295079-1.0.1.1-3qnOyFXGPSBjhrVdqqK0IvziOpGmsAXzCiUPRXMS.kFlgwr1J5vnB8nv7xJf2KSOeQRSe2uJYRy1Fh3TnkRH0qJR0qdtyHKKmzCSu1J2xxk; path=/; expires=Mon, 04-Aug-25 08:41:19 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=EZuOe6pdwq8J2WY0Rr6KHu_jJRgoPlod65cMq1Qj6u4-1754295079569-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c95551e903278-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 08:11:19,567 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-08-04 08:11:19,567 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 08:11:19,567 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 08:11:19,567 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 08:11:19,568 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 08:11:19,573 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 292, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
2025-08-04 08:12:05,508 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 08:12:07,796 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 08:12:07,835 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e4ae8dd9880>
2025-08-04 08:12:07,835 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7e4aea480e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 08:12:07,886 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7e4ae8d0f980>
2025-08-04 08:12:07,886 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 08:12:07,886 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 08:12:07,886 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 08:12:07,886 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 08:12:07,886 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 08:12:08,310 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Mon, 04 Aug 2025 08:12:08 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'144'), (b'Connection', b'keep-alive'), (b'CF-RAY', b'969c9685793c6882-NRT'), (b'mistral-correlation-id', b'01987423-5023-71f8-8acf-61aa9ebdf71b'), (b'x-kong-request-id', b'01987423-5023-71f8-8acf-61aa9ebdf71b'), (b'x-envoy-upstream-service-time', b'52'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'52'), (b'x-kong-proxy-latency', b'7'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=qZnK3MrRP1vEFXDSi_LYW3ttvhKsu1jMbw4.wqBHt9Q-1754295128-1.0.1.1-CTQXyTJn7zDcIDSkNQzCY5LDuJJew5ezB4odB4OArTxrMkiYXEWi0BkJYQbV5FAd.HQa4ei9.0x_LjNhtgu7eZNLjbc_.D7njr40T8sckFA; path=/; expires=Mon, 04-Aug-25 08:42:08 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=0AKY6e.zG2cL8uK5Qa4V961p14KOMZUYzIFJntzRuvc-1754295128296-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Server', b'cloudflare'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 08:12:08,311 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-08-04 08:12:08,311 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 08:12:08,311 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 08:12:08,311 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 08:12:08,311 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 08:12:08,315 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 285, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
2025-08-04 08:15:15,204 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 08:15:19,932 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 08:15:19,936 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a36130358b0>
2025-08-04 08:15:19,936 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7a3614158e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 08:15:19,945 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a3612af46e0>
2025-08-04 08:15:19,945 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 08:15:19,945 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 08:15:19,945 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 08:15:19,945 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 08:15:19,945 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 08:15:20,266 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Mon, 04 Aug 2025 08:15:20 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'144'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'01987426-3e13-71e1-8c3b-a75117fe09a0'), (b'x-kong-request-id', b'01987426-3e13-71e1-8c3b-a75117fe09a0'), (b'x-envoy-upstream-service-time', b'48'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'49'), (b'x-kong-proxy-latency', b'7'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=X2PwY8PLTPawNrSSBYwfGu86ZHl3cPqKfav3tC1xqeM-1754295320-1.0.1.1-NiSCGNxO36i6fJKUpP7Z0mPX8jNnJJmOkuijWJCpZcYJBxHqy_iYtcD7zSU7TELIZsT3rOn.0.CmeUPoJERNepFfPlkrg8eS62he2aw8A_Q; path=/; expires=Mon, 04-Aug-25 08:45:20 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=fY7v.kUaLazLQ7p4GqkXyThCJcyy1tyFPxsm14Qgju8-1754295320265-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c9b35acb2eaa2-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 08:15:20,266 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-08-04 08:15:20,266 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 08:15:20,267 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 08:15:20,267 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 08:15:20,267 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 08:15:20,271 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 294, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
2025-08-04 08:15:36,347 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-04 08:15:36,354 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-04 08:15:36,358 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7711a45b21e0>
2025-08-04 08:15:36,358 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7711a5c70e50> server_hostname='api.mistral.ai' timeout=120
2025-08-04 08:15:36,365 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7711a3b0cb00>
2025-08-04 08:15:36,365 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-04 08:15:36,365 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-04 08:15:36,365 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-04 08:15:36,365 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-04 08:15:36,365 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-04 08:15:36,696 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Mon, 04 Aug 2025 08:15:36 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'144'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'01987426-7e43-777a-a84f-c3ccede5abc5'), (b'x-kong-request-id', b'01987426-7e43-777a-a84f-c3ccede5abc5'), (b'x-envoy-upstream-service-time', b'45'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'46'), (b'x-kong-proxy-latency', b'6'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=hp8SwBg9EgYewQSuKT37kG7ZHKLiS2XmAZdYvu4hsrM-1754295336-1.0.1.1-kErVCvPGGRaiL8OvEokFYFv2xVAiyHgcY3.4W7Yz8VHqBEqneun9qHiVXE1vMs9SO2nTA50JGr446qkfHaacShwVQUVUw2RqpyPybiAppXI; path=/; expires=Mon, 04-Aug-25 08:45:36 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=F8YSFhhqgC11VFMwsfJQigIfmMsVmMSkHVe1_lxKaEQ-1754295336692-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'969c9b9c4e30ea19-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-04 08:15:36,698 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-08-04 08:15:36,698 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-04 08:15:36,699 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-04 08:15:36,699 - httpcore.http11 - DEBUG - response_closed.started
2025-08-04 08:15:36,700 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-04 08:15:36,718 - api.api_server - ERROR - 오류 발생: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 288, in chat_endpoint
    result = await agent_manager.chat(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/agent/llm_ollama.py", line 206, in chat
    raise Exception(result["error"])
Exception: ❌ An error occurred: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {"object":"error","message":"Service tier capacity exceeded for this model.","type":"service_tier_capacity_exceeded","param":null,"code":"3505"}
2025-08-05 04:13:22,741 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.044s]
2025-08-05 04:18:05,564 - httpcore.connection - DEBUG - connect_tcp.started host='api.mistral.ai' port=443 local_address=None timeout=120 socket_options=None
2025-08-05 04:18:05,617 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x705fa47d9580>
2025-08-05 04:18:05,617 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x705fa5e20e50> server_hostname='api.mistral.ai' timeout=120
2025-08-05 04:18:05,630 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x705fa47d9550>
2025-08-05 04:18:05,630 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-05 04:18:05,631 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-05 04:18:05,631 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-05 04:18:05,631 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-05 04:18:05,632 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-05 04:18:06,091 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 05 Aug 2025 04:18:06 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'mistral-correlation-id', b'01987873-674f-7ee4-9113-70e842f5b8f9'), (b'x-kong-request-id', b'01987873-674f-7ee4-9113-70e842f5b8f9'), (b'x-envoy-upstream-service-time', b'167'), (b'access-control-allow-origin', b'*'), (b'x-kong-upstream-latency', b'168'), (b'x-kong-proxy-latency', b'11'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=rleb_4_qBxIYCTSEiHJ.XeamU2six6tjsFFhrSJKqpM-1754367486-1.0.1.1-YAeEOJP82DElg8enEruzImiNwySndjl.Zf6s_qvClDYPbhXKw1yIqzKMTDzBITIOIKGxZPhG6FxoH1SjnQpM77gQxIlIwQfenpo0LtiiSQU; path=/; expires=Tue, 05-Aug-25 04:48:06 GMT; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Strict-Transport-Security', b'max-age=15552000; includeSubDomains'), (b'Set-Cookie', b'_cfuvid=bPZERelW_ozE56UCa3QWtWmmHHx5aaL9rqfxS81ZtUU-1754367486087-0.0.1.1-604800000; path=/; domain=.mistral.ai; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'96a37d113821c453-ICN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-08-05 04:18:06,093 - httpx - INFO - HTTP Request: POST https://api.mistral.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-05 04:18:06,093 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-05 04:18:12,090 - httpcore.http11 - DEBUG - response_closed.started
2025-08-05 04:18:12,090 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-05 04:18:12,091 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2025-08-05 04:18:12,168 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.076s]
2025-08-05 04:20:16,984 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 04:31:33,175 - api.api_server - ERROR - 오류 발생: OllamaAgentManager.__init__() got an unexpected keyword argument 'api_key'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 95, in startup_event
    agent_manager = OllamaAgentManager(
                    ^^^^^^^^^^^^^^^^^^^
TypeError: OllamaAgentManager.__init__() got an unexpected keyword argument 'api_key'
2025-08-05 04:34:05,562 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.044s]
2025-08-05 04:34:11,281 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-08-05 04:34:11,283 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x78f73e9774d0>
2025-08-05 04:34:11,283 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-05 04:34:11,283 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-05 04:34:11,283 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-05 04:34:11,284 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-05 04:34:11,284 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-05 04:36:14,691 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.011s]
2025-08-05 04:36:16,055 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-08-05 04:36:16,056 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x73069433bbc0>
2025-08-05 04:36:16,057 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-05 04:36:16,057 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-05 04:36:16,057 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-05 04:36:16,057 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-05 04:36:16,057 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-05 04:39:23,700 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 05 Aug 2025 04:39:23 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-08-05 04:39:23,700 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-08-05 04:39:23,701 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-05 04:39:23,701 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-05 04:39:23,701 - httpcore.http11 - DEBUG - response_closed.started
2025-08-05 04:39:23,701 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-05 04:39:23,833 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.129s]
2025-08-05 04:44:01,181 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.009s]
2025-08-05 04:49:02,873 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 04:59:28,040 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.051s]
2025-08-05 04:59:43,645 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:20:36,648 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.044s]
2025-08-05 06:20:42,897 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:25:57,712 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:26:00,438 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:26:02,226 - api.api_server - ERROR - 오류 발생: '\n  "image_description"'
Traceback (most recent call last):
  File "/home/ljy/python_code/MCP/ollama-mcp-chat/api/api_server.py", line 228, in chat_endpoint
    formatted_prompt = Prompt.format(
                       ^^^^^^^^^^^^^^
KeyError: '\n  "image_description"'
2025-08-05 06:26:18,237 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:26:19,300 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-08-05 06:26:19,302 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x767b6694ff50>
2025-08-05 06:26:19,302 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-05 06:26:19,303 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-05 06:26:19,303 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-05 06:26:19,303 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-05 06:26:19,303 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-05 06:28:24,417 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 05 Aug 2025 06:28:24 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-08-05 06:28:24,425 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-08-05 06:28:24,426 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-05 06:28:24,426 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-05 06:28:24,426 - httpcore.http11 - DEBUG - response_closed.started
2025-08-05 06:28:24,427 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-05 06:28:24,529 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.099s]
2025-08-05 06:31:22,914 - httpcore.connection - DEBUG - close.started
2025-08-05 06:31:22,915 - httpcore.connection - DEBUG - close.complete
2025-08-05 06:31:22,915 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-08-05 06:31:22,917 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x767b6694cc50>
2025-08-05 06:31:22,917 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-05 06:31:22,917 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-05 06:31:22,918 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-05 06:31:22,918 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-05 06:31:22,918 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-05 06:32:52,652 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 05 Aug 2025 06:32:52 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-08-05 06:32:52,653 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-08-05 06:32:52,653 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-05 06:32:52,654 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-05 06:32:52,654 - httpcore.http11 - DEBUG - response_closed.started
2025-08-05 06:32:52,654 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-05 06:32:52,798 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.142s]
2025-08-05 06:36:33,680 - httpcore.connection - DEBUG - close.started
2025-08-05 06:36:33,681 - httpcore.connection - DEBUG - close.complete
2025-08-05 06:36:33,681 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-08-05 06:36:33,682 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x767b66926390>
2025-08-05 06:36:33,682 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-05 06:36:33,683 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-05 06:36:33,683 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-05 06:36:33,683 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-05 06:36:33,683 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-05 06:38:03,260 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 05 Aug 2025 06:38:03 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-08-05 06:38:03,260 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-08-05 06:38:03,260 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-05 06:38:03,261 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-05 06:38:03,261 - httpcore.http11 - DEBUG - response_closed.started
2025-08-05 06:38:03,261 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-05 06:38:03,376 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.112s]
2025-08-05 06:38:44,769 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:38:49,790 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-08-05 06:38:49,791 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x73a8cf787e90>
2025-08-05 06:38:49,791 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-05 06:38:49,791 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-05 06:38:49,791 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-05 06:38:49,792 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-05 06:38:49,792 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-08-05 06:40:47,552 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 05 Aug 2025 06:40:47 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-08-05 06:40:47,553 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-08-05 06:40:47,553 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-08-05 06:40:47,554 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-05 06:40:47,554 - httpcore.http11 - DEBUG - response_closed.started
2025-08-05 06:40:47,554 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-05 06:40:47,802 - elastic_transport.transport - INFO - POST http://192.168.10.19:9201/chat_history/_doc [status:201 duration:0.245s]
2025-08-05 06:43:19,147 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:43:32,537 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:44:26,233 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:44:34,505 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:45:35,034 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.001s]
2025-08-05 06:45:45,429 - elastic_transport.transport - INFO - GET http://192.168.10.19:9201/ [status:200 duration:0.009s]
2025-08-05 06:45:49,410 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-08-05 06:45:49,410 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7a255b77ff50>
2025-08-05 06:45:49,410 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-08-05 06:45:49,410 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-05 06:45:49,410 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-08-05 06:45:49,410 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-05 06:45:49,411 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
